{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03078dd7",
   "metadata": {
    "id": "03078dd7"
   },
   "source": [
    "#  LangChain + 벡터 DB (FAISS) 기반으로 구성된 간단한 RAG 파이프라인 구현\n",
    "\n",
    "흐름 : [문서 리스트] → [임베딩 & FAISS 저장] → [유사 문서 검색] → [GPT 응답 생성]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3944e15e",
   "metadata": {
    "id": "3944e15e"
   },
   "outputs": [],
   "source": [
    "# 1. 문서 정의\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "model_descriptions = [\n",
    "    \"GPT 모델은 OpenAI가 개발한 대규모 언어 모델 시리즈로서, GPT-4 등 다양한 버전을 통해 다국어 텍스트 생성과 대화형 AI를 비롯한 폭넓은 활용이 가능합니다.\",\n",
    "    \"LLaMA 모델은 메타(Meta)에서 연구 목적으로 공개한 대규모 언어 모델로, 상대적으로 적은 자원으로도 고성능 텍스트 생성을 구현하도록 설계된 모델입니다.\",\n",
    "    \"DeepSeek 모델은 최근 대규모 파라미터와 고도화된 언어 이해 능력을 갖추고 있다는 점에서 주목받고 있으며, 질의응답이나 문장 생성 등에서 우수한 성능을 보이는 모델로 알려져 있습니다.\",\n",
    "    \"Gemma 모델은 첨단 NLP 기법을 적용해 다국어 텍스트 생성과 대화형 응용에 활용할 수 있도록 설계된 최신 대규모 언어 모델 중 하나입니다.\",\n",
    "    \"KoGPT2 (SKT-AI) 모델은 GPT-2 구조를 한국어 데이터셋에 맞춰 학습하여 가벼운 파라미터 수 대비 자연스러운 한국어 텍스트 생성이 가능한 모델입니다.\",\n",
    "    \"KoGPT6B (SKT-AI) 모델은 6B(60억) 파라미터를 갖춘 확장된 GPT-2 기반으로 대량의 한국어 텍스트를 학습해 긴 문맥 이해와 일관성 높은 문장 생성을 지원하는 모델입니다.\",\n",
    "    \"KoBART (SKT-AI) 모델은 BART 구조를 한국어로 최적화해 요약, 번역, 생성 등 다양한 NLP 태스크에 활용할 수 있는 인코더-디코더 기반 모델입니다.\",\n",
    "    \"KoT5 모델은 구글 T5를 한국어로 재학습하여 다방면의 텍스트 변환 및 생성 능력을 갖춘 범용 언어 모델입니다.\",\n",
    "    \"KoAlpaca 모델은 LLaMA 기반을 한국어 인스트럭션 튜닝으로 최적화해 보다 자연스럽고 유창한 한국어 문장 생성을 목표로 설계된 모델입니다.\",\n",
    "    \"Polyglot-Ko 시리즈 모델은 다중 언어 기반 학습 구조에서 한국어 최적화를 통해 텍스트 생성과 이해에 고른 성능을 보이는 모델입니다.\",\n",
    "    \"mBART 모델은 페이스북 AI의 멀티링구얼 BART를 기반으로 25개 이상의 언어에서 번역, 요약 등 다양한 텍스트 생성에 활용 가능한 모델입니다.\",\n",
    "    \"mT5 모델은 구글에서 공개한 다국어 T5로 100개 이상의 언어에서 질의응답, 번역, 요약 등 폭넓은 작업을 수행할 수 있는 범용 언어 모델입니다.\",\n",
    "    \"BLOOM 모델은 BigScience 프로젝트에서 공개한 대규모 멀티링구얼 언어 모델로 40개 이상의 언어에서 자연어 이해와 생성을 지원하는 오픈소스 모델입니다.\"\n",
    "]\n",
    "\n",
    "documents = [Document(page_content=text) for text in model_descriptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ahCSi6GMn8qF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11989,
     "status": "ok",
     "timestamp": 1752744494850,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "ahCSi6GMn8qF",
    "outputId": "9d705803-11c6-4e18-eb53-2c344616d157"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_openai\n",
      "  Downloading langchain_openai-0.3.28-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.68 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.3.68)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.95.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.9.0)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain_openai) (0.4.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain_openai) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain_openai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain_openai) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain_openai) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain_openai) (4.14.1)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain_openai) (2.11.7)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.86.0->langchain_openai) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain_openai) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain_openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain_openai) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.68->langchain_openai) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain_openai) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain_openai) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain_openai) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.68->langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.68->langchain_openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.68->langchain_openai) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.4.0)\n",
      "Downloading langchain_openai-0.3.28-py3-none-any.whl (70 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: langchain_openai\n",
      "Successfully installed langchain_openai-0.3.28\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5WudB4-p7LH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22324,
     "status": "ok",
     "timestamp": 1752744517195,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "e5WudB4-p7LH",
    "outputId": "6f255551-9a65-434b-a468-303a3f4f0160"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.68)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.26)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.5)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.7.14)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
      "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
      "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
      "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain-community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "Dzaa0I66qF0l",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13334,
     "status": "ok",
     "timestamp": 1752744530531,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "Dzaa0I66qF0l",
    "outputId": "25138ca4-ce4d-4449-fedb-2da718c6d782"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
      "Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.11.0.post1\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3fb910",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1790,
     "status": "ok",
     "timestamp": 1752659978264,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "0e3fb910",
    "outputId": "95693a08-0f20-48a4-e5ef-4858ca37ce3f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-8-1074053.py:7: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding_model = OpenAIEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "# 2. 임베딩 + FAISS 벡터스토어 구성\n",
    "# 각 문서 → 벡터로 임베딩 → FAISS에 저장\n",
    "# create_stuff_documents_chain() → 여러 문서를 단순히 “붙여서” GPT 입력으로 넘기는 방식\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(documents, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1b1336",
   "metadata": {
    "id": "6d1b1336"
   },
   "outputs": [],
   "source": [
    "# 3. 프롬프트 템플릿 + LLM 체인 정의\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "다음 문서 내용을 참고해서 질문에 간단히 답해주세요. 답변은 평문 텍스트로 작성해주세요.\n",
    "\n",
    "문서 내용:\n",
    "{context}\n",
    "\n",
    "질문:\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "stuff_prompt = PromptTemplate.from_template(prompt_template)\n",
    "qa_chain = create_stuff_documents_chain(llm=llm, prompt=stuff_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4ee5c0",
   "metadata": {
    "id": "cf4ee5c0"
   },
   "outputs": [],
   "source": [
    "# 4. Retrieval + RAG 체인 구성\n",
    "# retrieval : 벡터 기반 문서 검색기\n",
    "# question_answer_chain : GPT가 context + 질문을 받아 답변 생성\n",
    "# create_retrieval_chain : RAG 전체 파이프라인 구성\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "rag_chain = create_retrieval_chain(retriever, qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1affbd",
   "metadata": {
    "id": "6a1affbd"
   },
   "outputs": [],
   "source": [
    "# 5. 질문 실행\n",
    "query = \"상대적으로 적은 자원과 비용으로 활용할 수 있는 모델은 무엇이 있나요?\"\n",
    "result = rag_chain.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d50c4a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1752659979752,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "33d50c4a",
    "outputId": "baff459a-e717-4766-af3d-6568b5023f4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "답변:\n",
      " LLaMA 모델입니다.\n"
     ]
    }
   ],
   "source": [
    "# 6. 결과 출력\n",
    "print(\"\\n답변:\\n\", result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f992de",
   "metadata": {
    "id": "f9f992de"
   },
   "source": [
    "# MultiQueryRetriever를 활용한 LangChain RAG 코드\n",
    "\n",
    "### MultiQueryRetriever\n",
    "사용자 질문을 LLM을 통해 다양한 의미적 쿼리로 확장시킨 뒤, 이 여러 쿼리로 문서를 검색하고 병합하는 방식\n",
    "\n",
    "#### 내부 작동원리:\n",
    "1. 사용자 쿼리 → LLM을 통해 다양한 유사 질문들 생성 (예: 3~5개)\n",
    "2. 각각을 벡터로 임베딩 → 벡터 DB에 검색\n",
    "3. 여러 검색 결과를 통합 (중복 제거 포함)\n",
    "\n",
    "#### 사용하기 적합한 상황\n",
    "- 사용자가 모호한 질문을 던질 때\n",
    "- 문서가 다양한 표현으로 구성되어 있을 때\n",
    "- 기술적 키워드, 줄임말, 변형된 표현 등 포함 시\n",
    "- 정확성이 중요한 문서 QA나 논문 분석\n",
    "\n",
    "#### 주의할 것!\n",
    "- MultiQueryRetriever는 LLM을 사용하므로 속도가 느릴 수 있고, 비용이 발생할 수 있습니다 (OpenAI, Claude 등 사용 시).\n",
    "\n",
    "- 의미가 너무 확장되면 irrelevant document가 섞일 수 있습니다. → top_k 제한, 스코어 필터링으로 조절 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf6ab99",
   "metadata": {
    "id": "3cf6ab99"
   },
   "outputs": [],
   "source": [
    "# 1. 기본 설정 및 문서 준비\n",
    "# 모델 설명을 리스트로 정의하고 LangChain 문서로 변환\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "model_descriptions = [\n",
    "    \"GPT 모델은 OpenAI가 개발한 대규모 언어 모델 시리즈로서, GPT-4 등 다양한 버전을 통해 다국어 텍스트 생성과 대화형 AI를 비롯한 폭넓은 활용이 가능합니다.\",\n",
    "    \"LLaMA 모델은 메타(Meta)에서 연구 목적으로 공개한 대규모 언어 모델로, 상대적으로 적은 자원으로도 고성능 텍스트 생성을 구현하도록 설계된 모델입니다.\",\n",
    "    \"DeepSeek 모델은 최근 대규모 파라미터와 고도화된 언어 이해 능력을 갖추고 있다는 점에서 주목받고 있으며, 질의응답이나 문장 생성 등에서 우수한 성능을 보이는 모델로 알려져 있습니다.\",\n",
    "    \"Gemma 모델은 첨단 NLP 기법을 적용해 다국어 텍스트 생성과 대화형 응용에 활용할 수 있도록 설계된 최신 대규모 언어 모델 중 하나입니다.\",\n",
    "    \"KoGPT2 (SKT-AI) 모델은 GPT-2 구조를 한국어 데이터셋에 맞춰 학습하여 가벼운 파라미터 수 대비 자연스러운 한국어 텍스트 생성이 가능한 모델입니다.\",\n",
    "    \"KoGPT6B (SKT-AI) 모델은 6B(60억) 파라미터를 갖춘 확장된 GPT-2 기반으로 대량의 한국어 텍스트를 학습해 긴 문맥 이해와 일관성 높은 문장 생성을 지원하는 모델입니다.\",\n",
    "    \"KoBART (SKT-AI) 모델은 BART 구조를 한국어로 최적화해 요약, 번역, 생성 등 다양한 NLP 태스크에 활용할 수 있는 인코더-디코더 기반 모델입니다.\",\n",
    "    \"KoT5 모델은 구글 T5를 한국어로 재학습하여 다방면의 텍스트 변환 및 생성 능력을 갖춘 범용 언어 모델입니다.\",\n",
    "    \"KoAlpaca 모델은 LLaMA 기반을 한국어 인스트럭션 튜닝으로 최적화해 보다 자연스럽고 유창한 한국어 문장 생성을 목표로 설계된 모델입니다.\",\n",
    "    \"Polyglot-Ko 시리즈 모델은 다중 언어 기반 학습 구조에서 한국어 최적화를 통해 텍스트 생성과 이해에 고른 성능을 보이는 모델입니다.\",\n",
    "    \"mBART 모델은 페이스북 AI의 멀티링구얼 BART를 기반으로 25개 이상의 언어에서 번역, 요약 등 다양한 텍스트 생성에 활용 가능한 모델입니다.\",\n",
    "    \"mT5 모델은 구글에서 공개한 다국어 T5로 100개 이상의 언어에서 질의응답, 번역, 요약 등 폭넓은 작업을 수행할 수 있는 범용 언어 모델입니다.\",\n",
    "    \"BLOOM 모델은 BigScience 프로젝트에서 공개한 대규모 멀티링구얼 언어 모델로 40개 이상의 언어에서 자연어 이해와 생성을 지원하는 오픈소스 모델입니다.\"\n",
    "]\n",
    "\n",
    "documents = [Document(page_content=text) for text in model_descriptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c010586f",
   "metadata": {
    "id": "c010586f"
   },
   "outputs": [],
   "source": [
    "# 2. 벡터스토어 (FAISS + OpenAI 임베딩) 구축\n",
    "# 고속 벡터 검색을 위한 벡터 DB 구성\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(documents, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788be2a6",
   "metadata": {
    "id": "788be2a6"
   },
   "outputs": [],
   "source": [
    "# 3. LLM과 프롬프트 설정\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "다음 문서를 참고해서 질문에 간결하게 답해주세요. 제공된 문서에 없는 정보는 추론하지 마세요.\n",
    "\n",
    "문서 내용:\n",
    "{context}\n",
    "\n",
    "질문:\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "stuff_prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# create_stuff_documents_chain\n",
    "# 회수한 문서들을 LLM에게 전달해 응답 생성\n",
    "qa_chain = create_stuff_documents_chain(llm=llm, prompt=stuff_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e08699",
   "metadata": {
    "id": "41e08699"
   },
   "outputs": [],
   "source": [
    "# 4. LLM 기반 MultiQueryRetriever 사용\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "# LLM으로 질문을 다양한 표현으로 재작성하여 더 넓은 문서 검색 수행\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# 전체 Retrieval + QA 체인 구성\n",
    "rag_chain = create_retrieval_chain(\n",
    "    retriever=multi_query_retriever,\n",
    "    combine_docs_chain=qa_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c708c58",
   "metadata": {
    "id": "1c708c58"
   },
   "outputs": [],
   "source": [
    "# 5. 사용자 쿼리 실행\n",
    "query = \"LoRA를 활용했을 때 기존 방식과 비교해 어떤 이점이 있나요?\"\n",
    "\n",
    "result = rag_chain.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9f46fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1752659982776,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "8c9f46fa",
    "outputId": "efbcccfe-8da1-4643-f585-4ace3d0e25e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "답변:\n",
      " LoRA를 활용했을 때 기존 방식과 비교해 전송 거리가 더 멀고 배터리 수명이 더 길며 더 적은 전력을 사용한다는 이점이 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 6. 결과 출력\n",
    "print(\"\\n답변:\\n\", result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c788b799",
   "metadata": {
    "id": "c788b799"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c635baab",
   "metadata": {
    "id": "c635baab"
   },
   "source": [
    "#  LangChain 기반 RAG + 텍스트 파일(txt)\n",
    "\n",
    "#### 핵심 흐름 (RAG 파이프라인)\n",
    "1. 텍스트 → 문서 청크로 나누기\n",
    "2. 문서 청크 → 벡터로 임베딩\n",
    "3. FAISS 벡터 DB에 저장\n",
    "4. 질문 → 관련 문서 검색\n",
    "5. 관련 문서 + 질문 → LLM 답변 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d3f4ea",
   "metadata": {
    "id": "28d3f4ea"
   },
   "outputs": [],
   "source": [
    "# 1. 텍스트 분할기 및 데이터 로딩\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "file_path = '/content/drive/MyDrive/Projects/LLM/RAG/lora.txt'\n",
    "with open(file_path, encoding='utf-8') as f:\n",
    "    all_text = f.read()\n",
    "\n",
    "# RecursiveCharacterTextSplitter\n",
    "# : 기본적으로 문장 부호나 단락을 기준으로 최대한 의미 단위로 쪼개려고 함.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,          # 각 청크 최대 200자\n",
    "    chunk_overlap=20,        # 청크 간 20자 중복 | overlap을 줘서 문맥 단절을 방지\n",
    "    length_function=len,     # 길이 측정 함수\n",
    "    is_separator_regex=False # 구분자 단순 문자로 인식\n",
    ")\n",
    "\n",
    "# 청크 분할 수행\n",
    "# create_documents()\n",
    "# : 각 청크를 Document 객체로 생성\n",
    "texts = text_splitter.create_documents([all_text])\n",
    "documents = [Document(page_content=text.page_content) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43917550",
   "metadata": {
    "id": "43917550"
   },
   "outputs": [],
   "source": [
    "# 2. 임베딩 모델 및 벡터스토어 생성(FAISS)\n",
    "# FAISS\n",
    "# : Facebook에서 만든 빠른 유사도 검색 라이브러리\n",
    "# 각 문서를 벡터로 변환 후, FAISS 벡터 인덱스에 저장\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(documents, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92da1593",
   "metadata": {
    "id": "92da1593"
   },
   "outputs": [],
   "source": [
    "# 3. Retrieval-augmented QA 체인 구성\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "stuff_prompt = PromptTemplate.from_template(\"\"\"\n",
    "다음 문서 내용을 참고하여 질문에 대해 3문장 이내로 간략히 답을 제시해주세요. 답변에는 평문 텍스트를 사용해주세요.\n",
    "\n",
    "문서내용:\n",
    "{context}\n",
    "\n",
    "질문: {input}\n",
    "\n",
    "답변:\n",
    "\"\"\")\n",
    "\n",
    "qa_chain = create_stuff_documents_chain(llm=llm, prompt=stuff_prompt)\n",
    "retriever = vectorstore.as_retriever()\n",
    "chain = create_retrieval_chain(retriever, qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7334d022",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3281,
     "status": "ok",
     "timestamp": 1752659986870,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "7334d022",
    "outputId": "62ca7692-4e0d-4408-f5c4-c1cb15e08c7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[질문] lora를 활용했을 때, 기존 방식에 비해서 어떤 이점이 있는지 설명해주세요\n",
      "[답변] LORA를 활용하면 더 빠르고 유연한 실험 환경을 제공하여 적은 리소스로도 다양한 미세 조정 전략을 시도할 수 있습니다. 또한, 파라미터 수를 줄이면서도 성능을 유지하거나 향상시킬 수 있어 더 효율적인 결과를 얻을 수 있습니다. 미세 조정 과정에서 학습 속도가 향상되어 계산 비용을 줄이고 더 많은 실험을 수행할 수 있습니다.\n",
      "\n",
      "[질문] 상대적으로 적은 자원과 비용으로 활용할 수 있는 모델은 뭐가 있나요?\n",
      "[답변] LORA (Low-Rank Adaptation) 모델은 상대적으로 적은 자원과 비용으로 활용할 수 있는 모델입니다.\n"
     ]
    }
   ],
   "source": [
    "# 4. 질문 예시 및 결과 출력\n",
    "query_1 = \"lora를 활용했을 때, 기존 방식에 비해서 어떤 이점이 있는지 설명해주세요\"\n",
    "query_2 = \"상대적으로 적은 자원과 비용으로 활용할 수 있는 모델은 뭐가 있나요?\"\n",
    "\n",
    "for query in [query_1, query_2]:\n",
    "    result = chain.invoke({'input': query})\n",
    "    print(f\"\\n[질문] {query}\\n[답변] {result['answer']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea3f5b7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 831,
     "status": "ok",
     "timestamp": 1752659987707,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "1ea3f5b7",
    "outputId": "48e8ad1f-3cc8-49b6-a4fa-ad6cbb64da32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MMR 기반 답변] 대규모 LLM을 미세 조정할 수 있는 LORA 모델이 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 5. MMR(Maximal Marginal Relevance) 기반 검색으로 변경하여 재질문\n",
    "\n",
    "# 왜?\n",
    "# - 중복되지 않으면서도 관련 있는 문서들을 선택함\n",
    "#   → 덕분에 더 풍부하고 정확한 문맥을 LLM에 전달 가능\n",
    "# - 질의의 의도와 문맥을 더 잘 반영하기 위해\n",
    "\n",
    "retriever_mmr = vectorstore.as_retriever(search_type='mmr')\n",
    "chain_mmr = create_retrieval_chain(retriever_mmr, qa_chain)\n",
    "\n",
    "result_mmr = chain_mmr.invoke({'input': query_2})\n",
    "print(f\"\\n[MMR 기반 답변] {result_mmr['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee9509d",
   "metadata": {
    "id": "1ee9509d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7bc0298",
   "metadata": {
    "id": "d7bc0298"
   },
   "source": [
    "# LangChain + HuggingFace 기반 RAG (Retrieval-Augmented Generation) 구조 구축\n",
    "\n",
    "- Gemma LLM을 직접 로딩하여 응답 생성에 사용\n",
    "\n",
    "- HuggingFace Embedding 모델을 직접 래핑해서 커스텀 벡터 임베딩\n",
    "\n",
    "- FAISS 기반 벡터 검색기 구축\n",
    "\n",
    "- MMR (Max Marginal Relevance)을 이용한 리트리버 실험 포함\n",
    "\n",
    "- <흐름> : [질문] → [벡터 임베딩] → [관련 문서 검색] → [문서 + 질문 → 프롬프트] → [Gemma 모델 응답 생성]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E_R_uSXbqSLg",
   "metadata": {
    "id": "E_R_uSXbqSLg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d7d869",
   "metadata": {
    "id": "e5d7d869"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993376bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22410,
     "status": "ok",
     "timestamp": 1752660028532,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "993376bc",
    "outputId": "9139fd4b-8400-402b-b09e-fad6bde92bd7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "/tmp/ipython-input-26-57556909.py:18: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=gen_pipeline)\n"
     ]
    }
   ],
   "source": [
    "# 1. LLM (Gemma) 로딩\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to('cuda').eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "stop_token_ids = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")]\n",
    "\n",
    "gen_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=3000,\n",
    "    temperature=0.1,\n",
    "    do_sample=False,\n",
    "    eos_token_id=stop_token_ids\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=gen_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202c5bf1",
   "metadata": {
    "id": "202c5bf1"
   },
   "outputs": [],
   "source": [
    "# 2. 커스텀 임베딩 클래스 정의\n",
    "class CustomEmbeddings(Embeddings):\n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def _mean_pooling(self, hidden_states, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
    "        sum_embeddings = torch.sum(hidden_states * input_mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            with torch.no_grad():\n",
    "                inputs = self.tokenizer(text, padding=True, truncation=True, return_tensors='pt').to(self.device)\n",
    "                outputs = self.model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "                pooled = self._mean_pooling(outputs.hidden_states[-1], inputs['attention_mask'])\n",
    "                embeddings.append(pooled[0].cpu().tolist())\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "embedding_model = CustomEmbeddings(model, tokenizer, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e85caeb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1151,
     "status": "ok",
     "timestamp": 1752660029685,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "7e85caeb",
    "outputId": "aab75486-8185-49ac-d994-a838251ebb00"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# 3. 문서 정의 및 벡터스토어 생성\n",
    "model_descriptions = [\n",
    "    \"GPT 모델은 OpenAI가 개발한 대규모 언어 모델 시리즈로서, GPT-4 등 다양한 버전을 통해 다국어 텍스트 생성과 대화형 AI를 비롯한 폭넓은 활용이 가능합니다.\",\n",
    "    \"LLaMA 모델은 메타(Meta)에서 공개한 대규모 언어 모델로, 적은 자원으로 고성능 텍스트 생성을 구현할 수 있도록 설계된 모델입니다.\",\n",
    "    \"DeepSeek 모델은 고도화된 언어 이해 능력을 갖춘 대규모 모델로, 질의응답이나 문장 생성 등에서 우수한 성능을 보입니다.\",\n",
    "    \"Gemma 모델은 첨단 NLP 기법을 적용한 다국어 생성 및 대화용 최신 언어 모델입니다.\",\n",
    "    \"KoGPT2 모델은 GPT-2 구조를 한국어 데이터셋으로 학습하여 가볍고 자연스러운 한국어 생성이 가능합니다.\",\n",
    "    \"KoGPT6B 모델은 6B 파라미터로 긴 문맥 이해와 고성능 한국어 생성이 가능한 모델입니다.\",\n",
    "    \"KoBART 모델은 요약·번역 등 다양한 작업에 활용 가능한 한국어 특화 BART 모델입니다.\"\n",
    "]\n",
    "\n",
    "documents = [Document(page_content=text) for text in model_descriptions]\n",
    "vectorstore = FAISS.from_documents(documents, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f418fb",
   "metadata": {
    "id": "39f418fb"
   },
   "outputs": [],
   "source": [
    "# 4. 프롬프트 템플릿 및 QA 체인 정의\n",
    "template = \"\"\"\n",
    "다음 문서 내용을 참고하여 질문에 대해 3문장 이내로 간단히 답해주세요. 답변에는 평문 텍스트를 사용해주세요.\n",
    "\n",
    "문서내용:\n",
    "{context}\n",
    "\n",
    "질문:\n",
    "{input}\n",
    "\n",
    "답변:\n",
    "\"\"\"\n",
    "\n",
    "stuff_prompt = PromptTemplate(template=template)\n",
    "question_answer_chain = create_stuff_documents_chain(llm=llm, prompt=stuff_prompt)\n",
    "retriever = vectorstore.as_retriever()\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d90417",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37026,
     "status": "ok",
     "timestamp": 1752660066724,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "04d90417",
    "outputId": "3299b5e7-932c-4de9-b606-a47c03c0828f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'do_sample': True}. If this is not desired, please set these values explicitly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 한국어 생성이 가능한 모델은 어떤 것들이 있는지 알려줘\n",
      "답변: \n",
      "다음 문서 내용을 참고하여 질문에 대해 3문장 이내로 간단히 답해주세요. 답변에는 평문 텍스트를 사용해주세요.\n",
      "\n",
      "문서내용:\n",
      "DeepSeek 모델은 고도화된 언어 이해 능력을 갖춘 대규모 모델로, 질의응답이나 문장 생성 등에서 우수한 성능을 보입니다.\n",
      "\n",
      "LLaMA 모델은 메타(Meta)에서 공개한 대규모 언어 모델로, 적은 자원으로 고성능 텍스트 생성을 구현할 수 있도록 설계된 모델입니다.\n",
      "\n",
      "KoBART 모델은 요약·번역 등 다양한 작업에 활용 가능한 한국어 특화 BART 모델입니다.\n",
      "\n",
      "KoGPT2 모델은 GPT-2 구조를 한국어 데이터셋으로 학습하여 가볍고 자연스러운 한국어 생성이 가능합니다.\n",
      "\n",
      "질문:\n",
      "한국어 생성이 가능한 모델은 어떤 것들이 있는지 알려줘\n",
      "\n",
      "답변:\n",
      "KoGPT2 모델, KoBART 모델, KoLL 모델입니다.\n",
      "\n",
      "\n",
      "질문: 상대적으로 적은 자원과 비용으로 활용할 수 있는 모델은 뭐가 있나요?\n",
      "답변: \n",
      "다음 문서 내용을 참고하여 질문에 대해 3문장 이내로 간단히 답해주세요. 답변에는 평문 텍스트를 사용해주세요.\n",
      "\n",
      "문서내용:\n",
      "DeepSeek 모델은 고도화된 언어 이해 능력을 갖춘 대규모 모델로, 질의응답이나 문장 생성 등에서 우수한 성능을 보입니다.\n",
      "\n",
      "LLaMA 모델은 메타(Meta)에서 공개한 대규모 언어 모델로, 적은 자원으로 고성능 텍스트 생성을 구현할 수 있도록 설계된 모델입니다.\n",
      "\n",
      "KoBART 모델은 요약·번역 등 다양한 작업에 활용 가능한 한국어 특화 BART 모델입니다.\n",
      "\n",
      "KoGPT2 모델은 GPT-2 구조를 한국어 데이터셋으로 학습하여 가볍고 자연스러운 한국어 생성이 가능합니다.\n",
      "\n",
      "질문:\n",
      "상대적으로 적은 자원과 비용으로 활용할 수 있는 모델은 뭐가 있나요?\n",
      "\n",
      "답변:\n",
      "KoBART 모델, KoGPT2 모델, LLaMA 모델입니다.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. 질문 및 결과 출력\n",
    "queries = [\n",
    "    \"한국어 생성이 가능한 모델은 어떤 것들이 있는지 알려줘\",\n",
    "    \"상대적으로 적은 자원과 비용으로 활용할 수 있는 모델은 뭐가 있나요?\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    result = chain.invoke({'input': q})\n",
    "    print(f\"질문: {q}\\n답변: {result['answer']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de279cd5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 779,
     "status": "ok",
     "timestamp": 1752660067506,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "de279cd5",
    "outputId": "af7ed5d4-845a-409e-c524-deef0a11608d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMR 기반 리트리버 응답:\n",
      "\n",
      "다음 문서 내용을 참고하여 질문에 대해 3문장 이내로 간단히 답해주세요. 답변에는 평문 텍스트를 사용해주세요.\n",
      "\n",
      "문서내용:\n",
      "DeepSeek 모델은 고도화된 언어 이해 능력을 갖춘 대규모 모델로, 질의응답이나 문장 생성 등에서 우수한 성능을 보입니다.\n",
      "\n",
      "KoGPT2 모델은 GPT-2 구조를 한국어 데이터셋으로 학습하여 가볍고 자연스러운 한국어 생성이 가능합니다.\n",
      "\n",
      "LLaMA 모델은 메타(Meta)에서 공개한 대규모 언어 모델로, 적은 자원으로 고성능 텍스트 생성을 구현할 수 있도록 설계된 모델입니다.\n",
      "\n",
      "KoBART 모델은 요약·번역 등 다양한 작업에 활용 가능한 한국어 특화 BART 모델입니다.\n",
      "\n",
      "질문:\n",
      "상대적으로 적은 자원과 비용으로 활용할 수 있는 모델은 뭐가 있나요?\n",
      "\n",
      "답변:\n",
      "KoGPT2 모델, LLaMA 모델, KoBART 모델입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. MMR retriever 실험\n",
    "retriever_mmr = vectorstore.as_retriever(search_type='mmr')\n",
    "mmr_chain = create_retrieval_chain(retriever_mmr, question_answer_chain)\n",
    "result = mmr_chain.invoke({'input': queries[1]})\n",
    "print(f\"MMR 기반 리트리버 응답:\\n{result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "osVFh8dYrhBC",
   "metadata": {
    "id": "osVFh8dYrhBC"
   },
   "source": [
    "# 문서 임베딩 후 유사도 기반 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a64c0b9",
   "metadata": {
    "executionInfo": {
     "elapsed": 45999,
     "status": "ok",
     "timestamp": 1752744338546,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "8a64c0b9"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer    # 문장 수준 의미 임베딩\n",
    "from sklearn.metrics.pairwise import cosine_similarity   # 가장 의미 가까운 문서 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f6e53a5",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1752744338547,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "8f6e53a5"
   },
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"사과는 빨갛고 맛있는 과일입니다. 비타민 C가 풍부하여 건강에 좋습니다.\",\n",
    "    \"서울은 대한민국의 수도입니다. 한강이 도시를 가로지르며 아름다운 야경을 선사합니다.\",\n",
    "    \"기린은 목이 긴 동물로 아프리카 초원에 서식합니다. 주로 나뭇잎을 먹습니다.\",\n",
    "    \"커피는 전 세계적으로 인기 있는 음료입니다. 다양한 맛과 향이 있습니다.\",\n",
    "    \"독서는 지식을 넓히고 상상력을 자극하는 훌륭한 취미입니다. 마음의 양식이라고도 불립니다.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e023ae66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 511,
     "referenced_widgets": [
      "43618b1ef6e44d9d840bbb6390f037af",
      "bd2c0cf475ba4691859732c4f3070ea2",
      "ed3705cf32644b7d81ade5455a4425ef",
      "4c414862dbc443a1973d67f09377c75a",
      "954ba762f54f4563a5ca37ec9b7d42a4",
      "1ba4fc6d840548768f0d5d6de94718f4",
      "5c451699b86c4f7683ca4400902eb55f",
      "852061f72fe545149a6cb14b4ff8bf3e",
      "0e5e093da2924ba4941dc6dbdde31941",
      "b56c98c097e14a54bb6d24257f6e7671",
      "2e3dbd81a4564dcbb45e5dc5454a81fc",
      "69c73346a12b411780bb3633595fc161",
      "33a6cb6e43b34deb906770ff786ae186",
      "124975a849704a99b40114edff1d71ad",
      "67a7ec91bf854248b8690672f69d32c6",
      "34e1f6acfc7d47f18382cb4f190df1d0",
      "13522837916043fb9196ef2c56d9ba89",
      "eed4275099344cf2aa54eef171a3f0ee",
      "dcd111191dd4407abc86921b6b7d923c",
      "aa767c0d948d44ae84e5a2bcd9fd196d",
      "93fea475b6f74ba481d753782fce2d0e",
      "d44965c7fec54aa7a016183d01afce98",
      "151ed0e2386e48519c0a70eca4a6644a",
      "0ef430b45429413aaae8150948e16aba",
      "61c17f3458584aef8119543612c0dc79",
      "61ddb1aae47f4618b8aa51faa1277814",
      "f0eccc3f8e0e4598be05a291d03e75dd",
      "726d39d338c843ff8a54d51995f631c8",
      "3ca752904b0245be89e4cc6a671a5105",
      "ba8d097275fc4c029a02e59e9d72d3d3",
      "09e1200421bc40719a3b08b6c0e29698",
      "1c390b57d3de4cbcb1d9a6b109674a1d",
      "5baa931954c54b998a4717a51d82293c",
      "1d9c56834e6d417d9aa1d71a1272c825",
      "173ab474173e4d7da939917e8f85b65a",
      "c31212e429fa4d7491033f74ced08abd",
      "e164f4c9104d4002a8e258d284f1fe9b",
      "5319ddf796b546ba876b8a2b507af283",
      "86b0f9375703420e9d41177eed5fe744",
      "df76f8f85b5949a58cef60318284aa58",
      "46916604f96e4584800d3aed68a2721b",
      "c2cd42931d7e468a910901b628b9afcd",
      "06e964b32831498ca692a0c0e0bd01ac",
      "f289f65b73cf47ba8d9f96be94935f65",
      "848112da37f0465bbe89152f9b5777f2",
      "d2abc1a702914399bd7892754f2f3762",
      "d6a32af9bc77426abec0302c6c96c557",
      "99926a8835d346dab23e20bd480e3cf5",
      "a37eff2816b44b70a928ec18d0584630",
      "b132262a5d194f739f69a9a70754a774",
      "afa69e7d2b2c468ba985df63a0f57213",
      "03208e40686440a595020a38662adadf",
      "fb6017a01661441480e4c30977c356ba",
      "721fd0affdbb4ff2aefdf2504ee7a142",
      "388c1532e44140099cffe620311b8084",
      "58ff7e9ae5454cbfa75afad8440b085f",
      "f4e9588404d1450b9e5cc7cd8728dd5b",
      "4b48c136aea642b0852da8a587bb1d0b",
      "7309c41559624d29b02e0273722508cc",
      "a97e1e38231b467f89112a54557e76f4",
      "386fb0396fa74b44ac362c9b8a902903",
      "bf05a3c623e24efb8418f165cbc3c519",
      "509aa9a1d49d400688ec42ec7add0b05",
      "a6fa9026fac44cb7b03ffecc4a1bcadc",
      "996ac390459443948b256de532931012",
      "30892a6e610943bb90234969cf2e551e",
      "a035d72dd3dd411a9c03d5dcf54bdfb3",
      "24f2bbe95b054b1c95d44b59f0f5d666",
      "dfe083f1ba734f769c25d4acce076fd0",
      "3663d7bed0ae4a1eb45f648c55ccf9fd",
      "1069a22d2201423187d2086dda29503b",
      "4161d51ec68e42d19a7ee8b556a769ec",
      "19804468307b4b8abb67dafe133cecd6",
      "8b2067ff7b344c7982d310d72082d069",
      "380e11002a3243ea8a7585f2557b3d19",
      "c21757abb2e348599ac343f96c3f3410",
      "11ccb5682887456a95774ea9553155be",
      "f1b870fb4f594e05a88e8de334278a99",
      "83e6c6d09d9647de87fafec732cd75c2",
      "8072442555274d38bb9cbcbf7fcd1840",
      "e6fcc78404864118bfa289ee44517e91",
      "2f51a80c75144562b83836a8b4bff41f",
      "57b31f5e90f94fbe92308b80d44bf133",
      "2a3e37c3141749ee9661ba97b973bfb6",
      "70b6cd670b8440d7bf7c9bad2c8acbaf",
      "564e0dfe7b2c44f296f2c163f98b1257",
      "b4bcb000234c46ef902d9a92e3c29a2f",
      "1529d2743adc46d1bf4f2538d51e4931",
      "875b893edc85490ca12a9cc7ca3b66de",
      "5eaf167d187a4583aeb847a89bb99ac7",
      "47f1bc4021a2412bb36e09ed76be70d9",
      "07e225d7fd27478c9c716f0f86a0cbfc",
      "6b4fb27dfcc94f678deac06ff4560770",
      "71444295747a4049abcafdb4b8159d0c",
      "48197eb428684abe857a58fd690e7132",
      "e7fb810768fa4c81991753106fc0927a",
      "f9fbd062b16048a1b62603d7f887c6be",
      "0a11bec61e534ed896df6b8295fcd015",
      "fd69b12d001c4a13aeae6f6aa7643c01",
      "1bb208052d964f459293a94b70112b61",
      "34801b922396432d91a5f45285999a0d",
      "f595926ae4474a908c3a78a53407a9c3",
      "791cbb11f9244fe580d928a80afd2f1d",
      "402f669de3934ce2b61fb7be11a696f1",
      "5d8370dd751a4369b256bc3ada7b4769",
      "7dadc6ecfc584aa7ac02dde53c4958b9",
      "8e14b37530074bb59f4657d4324e2979",
      "10d8cc8fcbbb4704bb83879c5b51b607",
      "6941e48aea784e2fa9f8d392df134b31",
      "206f2425a8324ba3b7e5e6411ebe1e18",
      "86c823eb317a4319a789ed029c0f7c12",
      "17c7d30854994d52b717d165ae7e32c8",
      "7de2bb7de36f4564b5ef0e132d290fac",
      "33a4a6aaa9a6453d837a66e0a4a7ed85",
      "df5a2424c0ea40fea2cd0e0c13b297d2",
      "026d45245dfc40749986d9e401d196f3",
      "ab5f17afb7864088973a37aa01453ec0",
      "1d0b549078ca4326957af7f2a4d59431",
      "ecba6a5317434a9b99f03337591c5750",
      "1461f2acd5894aff9e941b0648bed98b",
      "ce2fe24fd9a44fdb9d3c5980175fece0"
     ]
    },
    "executionInfo": {
     "elapsed": 11076,
     "status": "ok",
     "timestamp": 1752744349619,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "e023ae66",
    "outputId": "18b150cb-c33a-4e2a-fc08-752aad3157b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43618b1ef6e44d9d840bbb6390f037af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c73346a12b411780bb3633595fc161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151ed0e2386e48519c0a70eca4a6644a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9c56834e6d417d9aa1d71a1272c825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848112da37f0465bbe89152f9b5777f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ff7e9ae5454cbfa75afad8440b085f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035d72dd3dd411a9c03d5dcf54bdfb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b870fb4f594e05a88e8de334278a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "875b893edc85490ca12a9cc7ca3b66de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb208052d964f459293a94b70112b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c823eb317a4319a789ed029c0f7c12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(5, 384)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SentenceTransformer로 문서 임베딩\n",
    "model_embedding = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "document_embeddings = model_embedding.encode(documents)\n",
    "document_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d340b26",
   "metadata": {
    "id": "2d340b26"
   },
   "source": [
    "document_embeddings.shape\n",
    "- (5, 384) = (문서 개수, 문서 하나 당 갖는 차원 수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bf3a5a3",
   "metadata": {
    "executionInfo": {
     "elapsed": 109,
     "status": "ok",
     "timestamp": 1752744349729,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "8bf3a5a3"
   },
   "outputs": [],
   "source": [
    "# 질의와 문서 간 유사도 계산\n",
    "query = \"아프리카 기린은 무엇을 먹나요.\"\n",
    "query_embedding = model_embedding.encode(query)\n",
    "similarities = cosine_similarity([query_embedding], document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fb65546",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1752744349732,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "5fb65546",
    "outputId": "147f1b62-2448-42e0-b4ee-a7c2199e8f86"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'기린은 목이 긴 동물로 아프리카 초원에 서식합니다. 주로 나뭇잎을 먹습니다.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가장 유사한 문서 선택\n",
    "most_similar_doc_index = similarities.argmax()\n",
    "retrieved_doc = documents[most_similar_doc_index]\n",
    "retrieved_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07e1f9a",
   "metadata": {
    "id": "e07e1f9a"
   },
   "source": [
    "# 선택된 문서와 질문을 기반으로 LLM 응답 생성 (Gemma 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d850c681",
   "metadata": {
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1752744349784,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "d850c681"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95888a0b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273,
     "referenced_widgets": [
      "6579208b5d964cdbabbf58a6ab0458e9",
      "1c87b2007f0d4c4eae13808c589e4be0",
      "fc2e513f4f5d4384b7b88a6d93c3d27e",
      "11f48c81fb2e47bbb94b75bf331ea5b0",
      "6c933b57cb514a34b545078b4c093c6a",
      "60f5a2a1643b43ceac1bfebd473c69ad",
      "a141052510af4cb1aca98689cc335346",
      "f78a3492cc9940b3a06cadc0a8f147b9",
      "d7bb9de97d784a8c97408fe8c5313600",
      "ae2ef924be0c457a983756710f19acb7",
      "c4556017c98845fbbda76eae7630a62e",
      "0b04384160874c52b530d072068539b5",
      "91de0a8faaa643b791cae4485e82b7e5",
      "ded1d4bfe02a409881b20522fef6d8ff",
      "fa203b8b225c452e9dd156754c263f52",
      "12814f5877494d31b1506fe04bb99be6",
      "1949417f34e94526b190cc7ea121eb4a",
      "d5578b289bdb4e19adc076ee7b231bcf",
      "8de15b4c07d1461abdf524c49b46f2f4",
      "fc0292ff9b2f44c48537b4abeb16e346",
      "55b38030ddd94bcb894b30290709173f",
      "7034576ee2334fe5a91b631704d00b14",
      "b79a892f7de9441584da3e35db2985b6",
      "9d813a4d14cc4b5fbd92c4c4a6532bac",
      "7fb09ebcf1064957b5f24b6510ed8f01",
      "39bd4bd1d7f74016ad22b5a1e2bc8b0d",
      "74f69696cc1b45a592760a03207988f0",
      "d4c9889b933941bcbcfed82cebc892c6",
      "ba621561b7ed47f29cb58814b9147500",
      "0aa22bc37cbb481a816660934fd2434d",
      "992128b14ec34534a054f3fe7c07fa44",
      "1288dfb6696f4c3c9a199ce366a833da",
      "21cc2de6db5441e8bcaa33da7d37692b",
      "ab4ea453d46f4fcf8defa20582f04d10",
      "961555820dd445ab98a00f4d36c21cb3",
      "9202c1d47aac4ad682f7064e989a5de9",
      "346bfce4198c411b8a5584e5ba6f666f",
      "dc81951585f64192a46c2201c48869d6",
      "265a15c87af44e58ba2ab6a8f6d7b680",
      "c29d796f60d04468817c8264e787e86b",
      "7df5bdbe04294320931538e151bda3b9",
      "33ec150769b549808c7c2e538f9c6b53",
      "b5080920f2d745d38fa3e40b671a6ffd",
      "20352b1fe5c14cd8a31d58b9b95b99be",
      "9afd40f01818429d9f7f551607682baa",
      "d7d5a3d7f7be4234afdf6b1d965fcdb2",
      "07a4dec778514678bcb5bc8a8dfe23a0",
      "ce2bae3d79b74715a0cbeeb5067ac98a",
      "4739437c832f42c1aa2d7b5e08b1cfaf",
      "b2e918e60ce64552b926788bcc5947b3",
      "f2013d5fafd04ebeafb22c71f63110d5",
      "b8bf0a2583734699b388b605610df7e2",
      "e0d47ec002244f5da6bc60bc732ee19d",
      "308a260a38bb495e8f2c22a7577dcd37",
      "8908598974284ee89b12845d0f7e1ae2",
      "4d8e3a1fcaf54d7c86d4e9c8bd4521b0",
      "474eafccb2864101bfe24cb6c9c77649",
      "1295d1381eb44cca96371f6d5c964733",
      "1e9b6d1892de476397ec18e60b4d5bc0",
      "8954732fb6024a2287a813d047cf6c81",
      "81ceb636430a43ceb05cee3dcc7b9b73",
      "5bcb7d1b68af4f56af197544626c08b2",
      "9879bb01e22146c7840ae5a4fbc68a01",
      "5b879febe8ee4252b6b6ebb5d8108c65",
      "5f16c88546de4dcdbc9644f5fa17f3f0",
      "860252c87693438f809d8f42014a3d22",
      "b46cb97e82f84e6abe826258285e0eec",
      "384e37eb3f824be09fcf5f0f83cf136c",
      "d5032b147eaf4fbf9c919f1f227f5e37",
      "49c58bc388e44f58836a21a402b7676f",
      "b9d3c2196c1146d28d58e53f2e596937",
      "4e0fca36cbad4cf58ecddb5e9559d8fb",
      "c5a53cefe2184d82bd2e0fdc190338d1",
      "f38e5dbbc1a948e4b4827b6b5252b47c",
      "4abe43f08f7644a08b407036867a365d",
      "4a0332c1e2eb4afc84765dc904073345",
      "3adc6ee85d4c4fefb3e296d4dadd5c0c",
      "d7f45d3dae4945fea57b2a9560ff5927",
      "bd2de674f9074a31a40d951f57337c81",
      "13146e97e7a542c9891e488eff425047",
      "510070ffad5b40a3ada7e5eea228e761",
      "cbcb2a4ebeff46c9895aab5795d3803f",
      "b0f4f83fdc884f0bb0dee73e14e996b8",
      "72ef5c3344024e6eb3c585e7bf9ec659",
      "2ca9572ed91d47e3a184f406fba28c55",
      "99a3ebe6e1254463b904a0895abd31bb",
      "719ce817269d47a3a8dd4483d9fd8616",
      "0030e9098472418a9fefa32eb119e0f3"
     ]
    },
    "executionInfo": {
     "elapsed": 54186,
     "status": "ok",
     "timestamp": 1752744447093,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "95888a0b",
    "outputId": "bac66b5b-2f8b-4263-bf20-e949f6c8979c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6579208b5d964cdbabbf58a6ab0458e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b04384160874c52b530d072068539b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b79a892f7de9441584da3e35db2985b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab4ea453d46f4fcf8defa20582f04d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9afd40f01818429d9f7f551607682baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d8e3a1fcaf54d7c86d4e9c8bd4521b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/899 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46cb97e82f84e6abe826258285e0eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f45d3dae4945fea57b2a9560ff5927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = 'google/gemma-3-1b-it'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n",
    "# model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2efe062f",
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1752744447120,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "2efe062f"
   },
   "outputs": [],
   "source": [
    "# 질의 프롬프트 생성\n",
    "# prompt = f\"정보: ... 질문: ...\" 형태는 instruct 기반 모델에서 권장\n",
    "context = retrieved_doc\n",
    "prompt = f\"다음 정보를 바탕으로 질문에 답해주세요:\\n정보: {context}\\n질문: {query}\\n답변:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a2bbe32",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1752744447122,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "7a2bbe32"
   },
   "outputs": [],
   "source": [
    "# 토큰화 및 생성\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5b5cbcf",
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1752744447159,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "b5b5cbcf"
   },
   "outputs": [],
   "source": [
    "# 출력 제어\n",
    "# 줄바꿈으로 멈추는 기준 정의\n",
    "# Stopping Criteria\n",
    "# : eos_token_id로도 가능하지만, 더 정교하게 통제하고 싶을 때 유용\n",
    "# : API inference시에도 응답 길이 통제를 위해 자주 사용됨\n",
    "class StopOnNewline(StoppingCriteria):\n",
    "    def __call__(self, input_ids, scores):\n",
    "        return input_ids[0, -1].item() == tokenizer.convert_tokens_to_ids(\"\\n\")\n",
    "\n",
    "stop_criteria = StoppingCriteriaList([StopOnNewline()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70831056",
   "metadata": {
    "executionInfo": {
     "elapsed": 22016,
     "status": "ok",
     "timestamp": 1752744469176,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "70831056"
   },
   "outputs": [],
   "source": [
    "# 응답 생성\n",
    "outputs = model.generate(\n",
    "    **input_ids,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    stopping_criteria=stop_criteria\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d7321e",
   "metadata": {
    "id": "05d7321e"
   },
   "source": [
    "#### ▶ generate() 옵션 정리\n",
    "| 옵션 이름                | 설명                                  |\n",
    "| -------------------- | ----------------------------------- |\n",
    "| `input_ids`          | 입력 시퀀스                              |\n",
    "| `max_new_tokens`     | 새로 생성할 최대 토큰 수                      |\n",
    "| `temperature`        | 생성 분포의 무작위성 조절 (0\\~1: 보수적, >1: 창의적) |\n",
    "| `top_p`              | 누적 확률 기반의 nucleus sampling (0.9 추천) |\n",
    "| `repetition_penalty` | 반복 방지 계수 (1.1\\~1.2 추천)              |\n",
    "| `eos_token_id`       | 종료 토큰 지정                            |\n",
    "| `stopping_criteria`  | 사용자 정의 중단 조건 (고급 사용)                |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19c50d3d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1752744469177,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "19c50d3d",
    "outputId": "71c9a532-c865-4445-af56-32cca201dc36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "답변:  기린은 주로 나뭇잎을 먹습니다.\n",
      "\n",
      "정보: 해바라기는 잎을 가지고 있고, 햇빛을 흡수하여 생명을 유지합니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 응답 디코딩\n",
    "input_len = input_ids['input_ids'].size(1)\n",
    "answer = tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)\n",
    "print(\"답변:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bd4332",
   "metadata": {
    "id": "87bd4332"
   },
   "source": [
    "# 이메일 요약 파이프라인 (LangChain + Pydantic Parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d80e3be",
   "metadata": {
    "executionInfo": {
     "elapsed": 2272,
     "status": "ok",
     "timestamp": 1752744532815,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "2d80e3be"
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser    # PydanticOutputParser를 이용하면 LLM 출력 포맷 강제 가능\n",
    "from langchain_core.prompts import PromptTemplate    # 프롬프트 구조화\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9301d023",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1752744532816,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "9301d023"
   },
   "outputs": [],
   "source": [
    "# JSON 구조 정의\n",
    "class EmailSummary(BaseModel):\n",
    "    person: str = Field(..., description='발신자 이름')\n",
    "    email: str = Field(..., description='발신자 이메일')\n",
    "    subject: str = Field(..., description='메일 제목')\n",
    "    summary: str = Field(..., description='메일 내용 요약')\n",
    "    date: str = Field(..., description='미팅 제안 날짜 및 시간')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7f92d04",
   "metadata": {
    "executionInfo": {
     "elapsed": 691,
     "status": "ok",
     "timestamp": 1752744533505,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "a7f92d04"
   },
   "outputs": [],
   "source": [
    "# 파서 및 LLM 설정\n",
    "parser = PydanticOutputParser(pydantic_object=EmailSummary)\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f3b0933",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1752744533526,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "0f3b0933"
   },
   "outputs": [],
   "source": [
    "# 프롬프트 템플릿 정의\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "메일 내용을 분석하여 다음 JSON 형식에 맞춰 요약해 주세요.\n",
    "미팅 제안 날짜와 시간이 있다면 해당 정보를 'date' 필드에 정확히 기입하고, 없다면 빈 문자열로 남겨주세요.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "메일 내용:\n",
    "{email_content}\n",
    "\"\"\",\n",
    "    input_variables=[\"email_content\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94035989",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1752744533538,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "94035989"
   },
   "outputs": [],
   "source": [
    "# 체인 구성\n",
    "email_chain = prompt_template | llm | parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d167bfee",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1752744533550,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "d167bfee"
   },
   "outputs": [],
   "source": [
    "# 테스트 이메일\n",
    "email_example = \"\"\"\n",
    "보낸 사람: 김영희 <younghee.kim@example.com>\n",
    "제목: 다음 주 미팅 관련 문의\n",
    "내용:\n",
    "안녕하세요, 김철수님.\n",
    "지난 번 논의드렸던 프로젝트 관련하여 다음 주 중으로 미팅을 가질 수 있을지 문의드립니다.\n",
    "가능하시다면 7월 23일 오전 10시나 7월 24일 오후 2시 중으로 시간을 제안해주시면 감사하겠습니다.\n",
    "미리 의제는 공유드리겠습니다.\n",
    "감사합니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef15bf78",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2467,
     "status": "ok",
     "timestamp": 1752744536018,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "ef15bf78",
    "outputId": "8720974e-51c1-49ad-e979-20f7123af19b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'person': '김영희', 'email': 'younghee.kim@example.com', 'subject': '다음 주 미팅 관련 문의', 'summary': '지난 번 논의드렸던 프로젝트 관련하여 다음 주 중으로 미팅을 가질 수 있는지 문의', 'date': '7월 23일 오전 10시 또는 7월 24일 오후 2시'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-27-2135934941.py:3: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  print(parsed_summary.dict())\n"
     ]
    }
   ],
   "source": [
    "# 요약 실행\n",
    "parsed_summary = email_chain.invoke({\"email_content\": email_example})\n",
    "print(parsed_summary.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc3af1d9",
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1752744536043,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "dc3af1d9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08186c75",
   "metadata": {
    "id": "08186c75"
   },
   "source": [
    "# 다양한 데이터 로더 예시 (LangChain 커뮤니티 제공)\n",
    "\n",
    "- PDF: PyPDF / PyMuPDF (텍스트 추출 품질 차이 존재)  →  PyMuPDF가 PyPDF보다 레이아웃 보존이 더 뛰어남\n",
    "\n",
    "- Web: bs4.SoupStrainer로 타겟 영역만 추출  →  WebBaseLoader: 웹 스크래핑 시 유용한 유연한 구조\n",
    "\n",
    "- ArXiv: 메타 정보 포함 논문 로딩  →  자동 논문 검색 및 분석에 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fc819c",
   "metadata": {
    "id": "d2fc819c"
   },
   "source": [
    "## PDF 로더: PyPDFLoader or PyMuPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "zH0w95frutRV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7471,
     "status": "ok",
     "timestamp": 1752744793077,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "zH0w95frutRV",
    "outputId": "759491d0-c9c1-4ed1-dd56-63adc72520c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.26.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea00baa0",
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1752744536083,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "ea00baa0"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b23bcb6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 676,
     "status": "ok",
     "timestamp": 1752744793757,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "1b23bcb6",
    "outputId": "1ae2557f-37b0-4ce5-8414-c03845f1cde8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 페이지 내용: 📄 InsightIQ B2B 솔루션 기술 자료 \n",
      "문서 버전: 1.0 최종 수정일: 2025 년 7 월 9 일 \n",
      " \n",
      "## 1. 제품 소개서: AI-Powered Analytics Dashboard \"InsightIQ\" \n",
      "1.1 제품 개요 \n",
      "InsightIQ 는 기업의 판매 데이터를 자동으로 분석하여 마케팅 전략 수립에 \n",
      "필요한 인사이트를 제공하는 B2B SaaS 솔루션입니다. 실시간 대시보드와 자동 \n",
      "리포팅 기능을 통해 데이터 기반 의사결정을 지원합니다. \n",
      "1.2 주요 기능 \n",
      "• \n",
      "실시간 데이터 대시보드: 매출, 고객 유입 경로, 전환율 등 핵심 지표를 \n",
      "실시간으로 시각화합니다. \n",
      "• \n",
      "AI 기반 이상 감지: 데이터 패턴을 학습하여 평소와 다른 이상 신호(매출 \n",
      "급감, 특정 제품 이탈률 증가 등)를 자동으로 감지하고 알림을 보냅니다. \n",
      "• \n",
      "자동 주간 리포트: 매주 월요일 오전 9 시, 핵심 성과 지표(KPI)와 주요 \n",
      "변동 사항을 요약한 리포트를 이메일로 자동 발송합니다. \n",
      "1.3 요금제 \n",
      "• \n",
      "Basic Plan: 월 10 만원, 사용자 3 명, 대시보드 5 개 \n",
      "• \n",
      "Pro Plan: 월 30 만원, 사용자 10 명, 대시보드 무제한, AI 이상 감지 기능 \n",
      "포함 \n",
      "• \n",
      "Enterprise Plan: 별도 문의, Pro Plan 기능 전체 포함, 전담 매니저 및 \n",
      "기술 지원 \n",
      "1.4 기술 요구사항 \n",
      "• \n",
      "데이터 연동: REST API 또는 CSV 파일 업로드를 통해 데이터 연동이 \n",
      "가능합니다. \n",
      "• \n",
      "지원 브라우저: Chrome, Safari, Firefox 최신 버전 \n",
      " \n",
      "## 2. 기술 지원: InsightIQ API 가이드 FAQ \n",
      "Q: API 인증은 어떻게 하나요?\n"
     ]
    }
   ],
   "source": [
    "pdf_path = '/content/drive/MyDrive/Projects/LLM/RAG/b2b_technical_solution.pdf'\n",
    "loader = PyMuPDFLoader(pdf_path)  # 또는 PyPDFLoader\n",
    "pdf_pages = loader.load()\n",
    "print(\"첫 페이지 내용:\", pdf_pages[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a021187",
   "metadata": {
    "id": "5a021187"
   },
   "source": [
    "## 웹 페이지 로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97d08033",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 442,
     "status": "ok",
     "timestamp": 1752744794203,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "97d08033",
    "outputId": "0b86ac6a-87e7-4ea5-f6d5-8ed62c2dbd66"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13573712",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 740,
     "status": "ok",
     "timestamp": 1752744794963,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "13573712",
    "outputId": "d9684858-db3d-4e34-d8c4-d1ce4f1b5a4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "웹 문서 개수: 1\n"
     ]
    }
   ],
   "source": [
    "web_loader = WebBaseLoader(\n",
    "    web_paths=[\"https://n.news.naver.com/article/437/0000378416\"],\n",
    "    bs_kwargs=dict(parse_only=bs4.SoupStrainer(\"div\", attrs={\"class\": [\"newsct_article _article_body\", \"media_end_head_title\"]})),\n",
    "    header_template={\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36\"\n",
    "    }\n",
    ")\n",
    "web_docs = web_loader.load()\n",
    "print(\"웹 문서 개수:\", len(web_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5021c1a0",
   "metadata": {
    "id": "5021c1a0"
   },
   "source": [
    "## ArXiv 논문 로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6CWHclsOu2p0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14228,
     "status": "ok",
     "timestamp": 1752744832869,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "6CWHclsOu2p0",
    "outputId": "c11c2622-273c-4f8d-a321-56c6e0ad4669"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv\n",
      "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting feedparser~=6.0.10 (from arxiv)\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.11/dist-packages (from arxiv) (2.32.3)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2025.7.14)\n",
      "Downloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=3466144cb10fc0555674df0dfbc524a1f3231c424d6cdbb52d01743b36255e87\n",
      "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
      "Successfully installed arxiv-2.2.0 feedparser-6.0.11 sgmllib3k-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "20a06139",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1752744832888,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "20a06139"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c696cde",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3976,
     "status": "ok",
     "timestamp": 1752744836862,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "0c696cde",
    "outputId": "833cb983-be56-4797-cb28-8af13f85573b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArXiv 문서 수: 3\n"
     ]
    }
   ],
   "source": [
    "arxiv_loader = ArxivLoader(query=\"large language model\", load_max_docs=3, load_all_available_meta=True)\n",
    "arxiv_docs = arxiv_loader.load()\n",
    "print(\"ArXiv 문서 수:\", len(arxiv_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5156d1",
   "metadata": {
    "executionInfo": {
     "elapsed": 34250,
     "status": "aborted",
     "timestamp": 1752744536357,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "4a5156d1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc0a269",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1752744536364,
     "user": {
      "displayName": "JE:ON",
      "userId": "01341189743429609390"
     },
     "user_tz": -540
    },
    "id": "adc0a269"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python(J_Note)",
   "language": "python",
   "name": "j_note"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
