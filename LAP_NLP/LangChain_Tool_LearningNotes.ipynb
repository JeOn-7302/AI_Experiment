{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a25eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24cadfb9",
   "metadata": {},
   "source": [
    "# Gemma 모델로 텍스트 요약하기 (Hugging Face + LangChain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f7487f",
   "metadata": {},
   "source": [
    "1. LLM 모델 로드 (pipeline, chatopenapi)\n",
    "2. Prompt 템플릿 생성\n",
    "3. Chains 으로 프롬프트와 모델을 연결\n",
    "5. Query 입력을 모델에 입력\n",
    "6. 결과를 postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94d10cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fb7e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files:   0%|          | 0/4 [11:23<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face에서 Gemma 모델 불러오기\n",
    "model_id = \"google/gemma-2-9b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.bfloat16,    # 메모리 절약용 float 타입\n",
    "    device_map='auto'    # GPU 자동 할당\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf111ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face 파이프라인 구성 (text generation)\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512, # 생성 최대 길이\n",
    "    temperature=0.0,    # 결정적 출력 (창의성 ↓, 정확성 ↑)\n",
    "    do_sample=False     # 무작위 샘플링 안 함\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87627819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain에서 사용할 수 있게 LLM wraping\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d113a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 템플릿 작성 (요약 지시 포함)\n",
    "summary_template = \"\"\"\n",
    "당신은 전문 요약가입니다.\n",
    "아래 주어진 텍스트를 50단어 이내로 간결하게 요약해 주세요.\n",
    "\n",
    "텍스트:\n",
    "{text_to_summarize}\n",
    "\n",
    "요약:\n",
    "\"\"\"\n",
    "summary_prompt = PromptTemplate.from_template(summary_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b3336c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMChain으로 요약 체인 구성\n",
    "summary_chain = LLMChain(llm=llm, prompt=summary_prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f59b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요약할 실제 텍스트 입력\n",
    "text_to_summarize = \"\"\"\\\n",
    "[데일리안 = 조인영 기자] 우리나라 앱 개발자가 앱 마켓사업자에게서 경험하는 주요 불공정 사례는 심사 지연과 등록 거부이며, 앱 내 결제(인앱결제)의 가장 큰 문제점은 ‘과도한 수수료’라고 인식하고 있는 것으로 나타났다.\n",
    "\n",
    "방송통신위원회와 한국인터넷진흥원은 11일 '전기통신사업법' 제22조의9(앱 마켓사업자의 의무 및 실태조사)에 따른 ‘2024년도 앱 마켓 실태조사 결과’를 발표했다.\n",
    "\n",
    "2023년도 국내 ‘앱 마켓’ 규모는 거래액 기준으로 8조1952억원으로 2022년 8조 7598억원 대비 6.4% 감소했다\n",
    "\n",
    "각 사업자별로 보면, 애플 앱스토어(10.1%)와 삼성전자 갤럭시스토어(6.3%)는 전년 대비 매출이 증가했으며, 구글 플레이(△10.1%)와 원스토어(△21.6%)는 감소했다.\n",
    "\n",
    "4개 앱 마켓사업자의 거래액 대비 수수료 비중은 약 14~26% 수준이며, 앱 등록 매출의 경우 애플 앱스토어는 약 9.4% 증가했고 구글 플레이는 약 12.9% 감소했다.\n",
    "\n",
    "또한 국내 앱 마켓에 등록된 전체 앱 수는 전년 대비 0.1% 증가한 531만8182개(각 앱 마켓별 중복 포함), 앱 개발자 수는 전년 대비 0.65% 적은 163만6655명(각 앱 마켓별 중복 포함)으로 나타났다.\n",
    "\n",
    "분야별 앱 등록 비중은 사진‧도구(26.1%), 라이프스타일(15.6%), 미디어‧출판(14.5%) 관련 앱이 전체의 56.2%를 차지했다.\n",
    "\n",
    "국내 앱 개발자가 이용하는 앱 마켓은 구글 플레이(96.4%), 애플 앱스토어(71.3%) 순(중복포함)이며, 매출액 비중도 구글 플레이가 67.5%로 가장 높았다. 그 를 애플 앱스토어(28.2%), 원스토어(2.9%), 갤럭시스토어(1.5%)가 따랐다.\n",
    "\n",
    "앱 개발자가 느끼는 주요 불공정 사례로는 앱 심사지연 경험(애플 앱스토어 6.8%, 구글 플레이 26.2%)이 가장 높았으며, 앱 등록 거부 경험(애플 20%, 구글 3%)과 앱 삭제 경험(구글 8.2%, 애플 3.2%)이 그 뒤를 이었다.\n",
    "\n",
    "앱 내 결제의 가장 큰 문제점은 ‘과도한 수수료’라고 답한 앱 개발자가 0.4%로 가장 높게 나타났으며, 다음으로 ‘환불 등 수익 정산의 불명확함’(11.6%), ‘결제 수단 선택 제한’(8.9%) 순으로 조사됐다.\n",
    "\n",
    "앱 개발자가 느낀 앱 최초 등록 과정상의 어려움으로는 ‘심사 기준이 확하지 않음’(구글 플레이 29.8%, 애플 앱스토어 29.6%), ‘질의에 대한 드백 지연’(각각 26.1%, 25.3%) 등이 꼽혔다.\n",
    "\n",
    "앱을 최초로 등록하기 위해 소요되는 심사기간은 구글 플레이는 등록 시 일 이내(25.6%)에 처리되는 경우가 많았으나, 애플 앱스토어는 6∼7일 이내(42.5%)로 나타났다.\n",
    "\n",
    "우리나라 국민이 가장 자주 이용하는 앱 마켓은 구글 플레이(67.2%), 애플 스토어(29.7%) 순이었으며, 해당 앱 마켓을 자주 이용하는 이유로는 ‘사용이 편리해서’(67.7%), ‘설치되어 있어서’(61.3%), ‘상품 수가 많아서’(33,5%) 등 으로 나타났다.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859457ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요약 실행\n",
    "summary_result = summary_chain.run(text_to_summarize)\n",
    "print(\"요약 결과:\", summary_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617b371d",
   "metadata": {},
   "source": [
    "# GPT-4o-mini로 상황에 맞는 모델 추천받기 (ChatOpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b47244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI 모델(GPT-4o-mini) 사용\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee92a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 정의 (GPT-4o-mini, 창의성 없는 결정적 출력)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d886f495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대화형 프롬프트 구성 (system + user 역할 분리)\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 질문의 상황에 적절한 인공지능 모델을 찾아주는 AI 전문가입니다.\"),\n",
    "    (\"user\", \"다음 상황에 맞는 모델을 찾아줘 \\n{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cf774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMChain 구성\n",
    "chat_chain = LLMChain(llm=llm, prompt=chat_template, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8747d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 질문 입력 및 실행\n",
    "user_input = \"우리는 CCTV에 범죄 상황을 인지하기 위한 기능을 넣으려고 합니다.\"\n",
    "response = chat_chain.run(input=user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f4e5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPT 추천 결과:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9fbfde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "937bb3c4",
   "metadata": {},
   "source": [
    "# 날짜 계산 Agent (Tool 1개)\n",
    ": LangChain의 Agent 기능을 활용해서 도구(tool)를 사용하는 지능형 에이전트를 만드는 실습 예제\n",
    "\n",
    ":특히, 내일 날짜를 알려줘라는 요청에 대해, 모델이 직접 계산하지 않고, **직접 정의한 함수(get_tomorrow_data)** 를 호출해서 답하는 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f09507",
   "metadata": {},
   "source": [
    "#### 전체 코드 흐름\n",
    "① GPT 모델 정의 (ChatOpenAI)\n",
    "\n",
    "② 시스템 프롬프트 정의\n",
    "\n",
    "③ 날짜 계산 함수 정의 (Tool 등록)\n",
    "\n",
    "④ Tool 기반 Agent 생성\n",
    "\n",
    "⑤ 사용자 입력 처리 및 함수 자동 호출\n",
    "\n",
    "⑥ 결과 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09cd5f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent, tool, create_react_agent\n",
    "from datetime import datetime, timedelta\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc71ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 모델 불러오기\n",
    "llm = ChatOpenAI(model = \"gpt-4o-mini\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e6d6fd",
   "metadata": {},
   "source": [
    ": temperature=0.0: 가장 안정적인 응답 유도 (창의성 ↓, 정확성 ↑)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6cde56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 에이전트용 프롬프트 정의\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"당신은 다양한 도구를 활용해서 질문에 답변할 수 있습니다.\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"placeholder\", \"{agent_scratchpad}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53660851",
   "metadata": {},
   "source": [
    "- placeholder : 에이전트 내부에서 사용하는 scratchpad (도구 호출 관련 기록 저장)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "175771a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], optional_variables=['agent_scratchpad'], input_types={'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x15d6b8d30>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'agent_scratchpad': []}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='당신은 다양한 도구를 활용해서 질문에 답변할 수 있습니다.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad', optional=True)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a4b8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. tool 함수 정의\n",
    "@tool\n",
    "# 내일 날짜를 반환하는 간단한 함수\n",
    "def get_tomorrow_data() -> str:\n",
    "    \"\"\" return tomorrow datetime in format of %Y-%m-%d \"\"\"\n",
    "    return (datetime.now() + timedelta(days=1)).strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1b5a87",
   "metadata": {},
   "source": [
    "- @tool 데코레이터 : 해당 함수를 LangChain에서 에이전트가 호출 가능한 도구로 등록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae3255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 도구 리스트 등록\n",
    "# 여러 개의 도구를 등록할 수 있다. (여기선 1개)\n",
    "tools = [get_tomorrow_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4842c9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Agent 생성\n",
    "# create_tool_calling_agent() : \n",
    "# 이 에이전트는 질문을 이해하고 적절한 도구를 고른 후 자동 실행\n",
    "agent = create_tool_calling_agent(\n",
    "    llm = llm,\n",
    "    tools = tools,\n",
    "    prompt = prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04d7ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. AgentExecutor로 실행기 구성\n",
    "# AgentExecutor : \n",
    "# 실제로 사용자의 입력을 받고 응답을 처리하는 실행 엔진\n",
    "agent_executor = AgentExecutor(\n",
    "    agent = agent,\n",
    "    tools = tools,\n",
    "    verbose=True,   # 실행 과정 자세히 출력\n",
    "    handle_parsing_errors = True    # 오류 발생 시 자동 처리\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e68ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_tomorrow_data` with `{}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m2025-07-17\u001b[0m\u001b[32;1m\u001b[1;3m내일의 날짜는 2025년 7월 17일입니다.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': '내일의 날짜를 알려줘.', 'output': '내일의 날짜는 2025년 7월 17일입니다.'}\n"
     ]
    }
   ],
   "source": [
    "# 7. 사용자 입력 → 에이전트 실행\n",
    "# invoke()를 사용해 입력을 전달하고 결과를 받음.\n",
    "result = agent_executor.invoke({\"input\": \"내일의 날짜를 알려줘.\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288207f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61fc3b4d",
   "metadata": {},
   "source": [
    "# LangChain Agents, Tool 2개\n",
    "**두 함수를 모두 사용해 하나의 응답 만들기**\n",
    "- get_tomorrow() → 내일 날짜 반환\n",
    "- get_current_time() → 현재 시각 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3c7d30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.agents import tool, create_tool_calling_agent, AgentExecutor\n",
    "\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5518fad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. LLM 정의 (GPT-4o-mini 사용)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82addfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Prompt 템플릿 정의\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 도구를 활용해 사용자 질문에 답변하는 똑똑한 AI입니다.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bbd52a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 첫 번째 도구: 내일 날짜 반환\n",
    "@tool\n",
    "def get_tomorrow() -> str:\n",
    "    \"\"\"내일 날짜를 YYYY-MM-DD 형식으로 반환합니다.\"\"\"\n",
    "    return (datetime.now() + timedelta(days=1)).strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bef9ffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 두 번째 도구: 현재 시간 반환\n",
    "@tool\n",
    "def get_current_time() -> str:\n",
    "    \"\"\"현재 시간을 HH:MM:SS 형식으로 반환합니다.\"\"\"\n",
    "    return datetime.now().strftime(\"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8581213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 도구 목록 정의\n",
    "tools = [get_tomorrow, get_current_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c80601ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 에이전트 생성\n",
    "agent = create_tool_calling_agent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a89a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 실행기 설정\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68cf2497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. 사용자 입력\n",
    "query = \"내일 날짜랑 지금 시간을 알려줘.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "370d1e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_tomorrow` with `{}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m2025-07-17\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_current_time` with `{}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m16:27:02\u001b[0m\u001b[32;1m\u001b[1;3m내일 날짜는 2025-07-17이고, 현재 시간은 16:27:02입니다.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      " 답변:\n",
      " 내일 날짜는 2025-07-17이고, 현재 시간은 16:27:02입니다.\n"
     ]
    }
   ],
   "source": [
    "# 9. 실행\n",
    "result = agent_executor.invoke({\"input\": query})\n",
    "print(\"\\n 답변:\\n\", result[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59094d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3219a600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f9426a7",
   "metadata": {},
   "source": [
    "# LangChain에서 ReAct 에이전트 구조를 활용\n",
    "사용자의 자연어 질문에 대해 도구(tool)를 순차적으로 선택·실행하면서 답을 만들어내는 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc8d246",
   "metadata": {},
   "source": [
    "<흐름>\n",
    "\n",
    "Question: 내일의 날씨를 알려줘\n",
    "<br>Thought: 내일 날짜를 알아야 한다\n",
    "<br>Action: get_tomorrow_data\n",
    "<br>Action Input: 없음\n",
    "<br>Observation: 2025-07-17\n",
    "\n",
    "Thought: 해당 날짜의 날씨를 알아야 한다\n",
    "<br>Action: get_weather\n",
    "<br>Action Input: 2025-07-17\n",
    "<br>Observation: 맑음\n",
    "\n",
    "Thought: 이제 최종 답을 알고 있습니다\n",
    "<br>Final Answer: 내일(2025-07-17)의 날씨는 맑음입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cbcbd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import tool, create_react_agent, AgentExecutor\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cda9216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. LLM 설정\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a37e1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 도구(tool) 정의\n",
    "@tool\n",
    "def get_tomorrow_data() -> str:\n",
    "    \"\"\"내일 날짜를 YYYY-MM-DD 형식으로 반환합니다.\"\"\"\n",
    "    return (datetime.now() + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "@tool\n",
    "def get_weather(date: str) -> str:\n",
    "    \"\"\"지정한 날짜의 날씨를 반환합니다. (예시: 항상 '맑음')\"\"\"\n",
    "    return '맑음'\n",
    "\n",
    "tools = [get_tomorrow_data, get_weather]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f87059f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ReAct 형식의 프롬프트 템플릿\n",
    "template = \"\"\" 다음 질문들에 최선을 다해서 답해주세요. 다음 도구들을 사용할 수 있습니다:\n",
    "\n",
    "{tools}\n",
    "\n",
    "다음 형식을 사용하세요:\n",
    "\n",
    "Question: 답해야 할 입력 질문\n",
    "Thought : 무엇을 해야 할지 항상 고민하세요\n",
    "Action : 수행할 작업, [{tool_names}] 중 하나여야 합니다\n",
    "Action Input : 작업에 대한 입력\n",
    "Observation: 작업의 결과\n",
    "... (위의 Thought/Action/Action Input/Observation 단계는 N 번 반복될 수 있습니다)\n",
    "Thought : 이제 최종 답을 알고 있습니다\n",
    "Final Answer: 원래 질문에 대한 최종 답변\n",
    "\n",
    "시작!\n",
    "\n",
    "Question: {input}\n",
    "Though: {agent_scratchpad}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27e17532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ReAct 에이전트 생성\n",
    "agent = create_react_agent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67f631b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 에이전트 실행기 설정\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True  # 내부 Thought/Action 흐름 출력\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50553c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m내일의 날짜를 알아야 내일의 날씨를 확인할 수 있습니다.  \n",
      "Action: get_tomorrow_data  \n",
      "Action Input: 없음  \n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m2025-07-17\u001b[0m\u001b[32;1m\u001b[1;3m이제 내일의 날짜를 알았으니, 그 날짜의 날씨를 확인해야 합니다.  \n",
      "Action: get_weather  \n",
      "Action Input: 2025-07-17  \n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m맑음\u001b[0m\u001b[32;1m\u001b[1;3m이제 최종 답을 알고 있습니다.  \n",
      "Final Answer: 내일의 날씨는 맑음입니다.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 6. 사용자 입력 실행\n",
    "query = \"내일의 날씨를 알려줘.\"\n",
    "result = agent_executor.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d7420d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "답변:\n",
      " 내일의 날씨는 맑음입니다.\n"
     ]
    }
   ],
   "source": [
    "# 7. 결과 출력\n",
    "print(\"\\n답변:\\n\", result[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4ad7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a1d9359",
   "metadata": {},
   "source": [
    "# 웹 검색 도구 - DuckDuckGoSearchResults\n",
    "LangChain ReAct 기반 에이전트에 DuckDuckGo 웹 검색 도구를 연결"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc9e56a",
   "metadata": {},
   "source": [
    "<실행 흐름>\n",
    "- Question: 최근 멀티헤드 어텐션을 변형한 아키텍처를 찾아주세요\n",
    "- Thought: 최신 기술 동향을 찾기 위해 검색이 필요합니다.\n",
    "- Action: DuckDuckGoSearchResults\n",
    "- Action Input: recent variants of multi-head attention\n",
    "- Observation: ...검색 결과 목록...\n",
    "- Thought: 정보를 바탕으로 답변할 수 있습니다.\n",
    "- Final Answer: 최근 변형된 어텐션 아키텍처로는 Linformer, Performer, Longformer 등이 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed0e6f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_react_agent, tool\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e4e5a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. LLM 설정 (gpt-4o-mini 사용)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "024b40c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/J_Note/lib/python3.9/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 2. 프롬프트 템플릿 (ReAct 스타일) 가져오기\n",
    "prompt = hub.pull(\"hwchase17/react\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc0895db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 검색 도구 정의 (DuckDuckGo)\n",
    "search_tool = DuckDuckGoSearchResults()\n",
    "tools = [search_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e78b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ReAct 에이전트 생성\n",
    "agent = create_react_agent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0903cb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 에이전트 실행기 구성\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,                # 사고과정 출력\n",
    "    handle_parsing_errors=True  # 응답 파싱 오류 자동 처리\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34e25d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m최근 멀티헤드 어텐션을 변형한 아키텍처에 대한 정보를 찾기 위해 검색해보겠습니다.  \n",
      "Action: duckduckgo_results_json  \n",
      "Action Input: \"recent architectures modified multi-head attention\"  \u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/J_Note/lib/python3.9/site-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3msnippet: Jan 22, 2025 · This study demonstrates that CNNs and ViT architectures stem from a unified underlying learning mechanism, which quantitatively measures the single-nodal performance …, title: Unified CNNs and transformers underlying learning mechanism …, link: https://arxiv.org/abs/2501.12900, snippet: 1 day ago · This paper presents a new architecture for multimodal sentiment analysis exploiting hierarchical cross-modal attention mechanisms, as well as two parallel lanes for audio analysis., title: Hierarchical cross-modal attention and dual audio pathways for …, link: https://www.nature.com/articles/s41598-025-09000-3, snippet: Jun 5, 2025 · The Multi-DConv-Head Attention (MDHA) addresses this limitation by integrating depthwise convolutions within the multi-head attention framework. This architectural innovation …, title: Multi-DConv-Head Attention: A Deep Learning Inovation for …, link: https://serp.ai/posts/multi-dconv-head-attention/, snippet: Feb 13, 2025 · Multi-head attention extends self-attention by splitting the input into multiple heads, enabling the model to capture diverse relationships and patterns. Instead of using a single set of …, title: Multi-Head Attention Mechanism - GeeksforGeeks, link: https://www.geeksforgeeks.org/nlp/multi-head-attention-mechanism/\u001b[0m\u001b[32;1m\u001b[1;3m이제 멀티헤드 어텐션을 변형한 아키텍처에 대한 정보를 찾았습니다. 특히 \"Multi-DConv-Head Attention\"이라는 새로운 아키텍처가 깊이별 합성곱을 멀티헤드 어텐션 프레임워크에 통합하여 한계를 극복하는 혁신을 제시하고 있습니다.  \n",
      "Final Answer: 최근 멀티헤드 어텐션을 변형한 아키텍처로는 \"Multi-DConv-Head Attention\"이 있으며, 이는 깊이별 합성곱을 통합하여 성능을 향상시키는 혁신적인 접근법입니다.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 6. 사용자 질문 실행\n",
    "query = \"최근 멀티헤드 어텐션을 변형한 아키텍처를 찾아주세요\"\n",
    "result = agent_executor.invoke({\"input\": query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a28ccc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "답변:\n",
      " 최근 멀티헤드 어텐션을 변형한 아키텍처로는 \"Multi-DConv-Head Attention\"이 있으며, 이는 깊이별 합성곱을 통합하여 성능을 향상시키는 혁신적인 접근법입니다.\n"
     ]
    }
   ],
   "source": [
    "# 7. 결과 출력\n",
    "print(\"\\n답변:\\n\", result[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a4b29b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5afe15f5",
   "metadata": {},
   "source": [
    "# 논문 검색 도구 - Arxiv\n",
    "사용자가 묻는 최신 연구 트렌드(예: 멀티헤드 어텐션 변형)에 대해 실제 논문 검색을 통해 응답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "815abb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain.agents import (\n",
    "    AgentExecutor,\n",
    "    create_react_agent,\n",
    "    load_tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13a508e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. LLM 설정 (ChatGPT 기반, 창의성 있는 답변 유도)\n",
    "llm = ChatOpenAI(temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75f019c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/J_Note/lib/python3.9/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 2. ReAct 프롬프트 템플릿 불러오기\n",
    "prompt = hub.pull(\"hwchase17/react\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc6f0a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Arxiv 검색 도구 로드\n",
    "tools = load_tools([\"arxiv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45b34d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ReAct 기반 에이전트 생성\n",
    "agent = create_react_agent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db0c5a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 에이전트 실행기 설정\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,                # 사고 과정 출력\n",
    "    handle_parsing_errors=True  # 파싱 오류 자동 처리\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "518cea3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should search for recent papers on variations of multi-head attention models in NLP.\n",
      "Action: arxiv\n",
      "Action Input: multi-head attention NLP variations after 2023\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/J_Note/lib/python3.9/site-packages/langchain_community/utilities/arxiv.py:103: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  return self.arxiv_search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3mPublished: 2024-08-02\n",
      "Title: Should We Attend More or Less? Modulating Attention for Fairness\n",
      "Authors: Abdelrahman Zayed, Goncalo Mordido, Samira Shabanian, Sarath Chandar\n",
      "Summary: The advances in natural language processing (NLP) pose both opportunities and\n",
      "challenges. While recent progress enables the development of high-performing\n",
      "models for a variety of tasks, it also poses the risk of models learning\n",
      "harmful biases from the data, such as gender stereotypes. In this work, we\n",
      "investigate the role of attention, a widely-used technique in current\n",
      "state-of-the-art NLP models, in the propagation of social biases. Specifically,\n",
      "we study the relationship between the entropy of the attention distribution and\n",
      "the model's performance and fairness. We then propose a novel method for\n",
      "modulating attention weights to improve model fairness after training. Since\n",
      "our method is only applied post-training and pre-inference, it is an\n",
      "intra-processing method and is, therefore, less computationally expensive than\n",
      "existing in-processing and pre-processing approaches. Our results show an\n",
      "increase in fairness and minimal performance loss on different text\n",
      "classification and generation tasks using language models of varying sizes.\n",
      "WARNING: This work uses language that is offensive.\n",
      "\n",
      "Published: 2023-11-26\n",
      "Title: Probabilistic Transformer: A Probabilistic Dependency Model for Contextual Word Representation\n",
      "Authors: Haoyi Wu, Kewei Tu\n",
      "Summary: Syntactic structures used to play a vital role in natural language processing\n",
      "(NLP), but since the deep learning revolution, NLP has been gradually dominated\n",
      "by neural models that do not consider syntactic structures in their design. One\n",
      "vastly successful class of neural models is transformers. When used as an\n",
      "encoder, a transformer produces contextual representation of words in the input\n",
      "sentence. In this work, we propose a new model of contextual word\n",
      "representation, not from a neural perspective, but from a purely syntactic and\n",
      "probabilistic perspective. Specifically, we design a conditional random field\n",
      "that models discrete latent representations of all words in a sentence as well\n",
      "as dependency arcs between them; and we use mean field variational inference\n",
      "for approximate inference. Strikingly, we find that the computation graph of\n",
      "our model resembles transformers, with correspondences between dependencies and\n",
      "self-attention and between distributions over latent representations and\n",
      "contextual embeddings of words. Experiments show that our model performs\n",
      "competitively to transformers on small to medium sized datasets. We hope that\n",
      "our work could help bridge the gap between traditional syntactic and\n",
      "probabilistic approaches and cutting-edge neural approaches to NLP, and inspire\n",
      "more linguistically-principled neural approaches in the future.\n",
      "\n",
      "Published: 2023-02-03\n",
      "Title: Improving Interpretability via Explicit Word Interaction Graph Layer\n",
      "Authors: Arshdeep Sekhon, Hanjie Chen, Aman Shrivastava, Zhe Wang, Yangfeng Ji, Yanjun Qi\n",
      "Summary: Recent NLP literature has seen growing interest in improving model\n",
      "interpretability. Along this direction, we propose a trainable neural network\n",
      "layer that learns a global interaction graph between words and then selects\n",
      "more informative words using the learned word interactions. Our layer, we call\n",
      "WIGRAPH, can plug into any neural network-based NLP text classifiers right\n",
      "after its word embedding layer. Across multiple SOTA NLP models and various NLP\n",
      "datasets, we demonstrate that adding the WIGRAPH layer substantially improves\n",
      "NLP models' interpretability and enhances models' prediction performance at the\n",
      "same time.\u001b[0m\u001b[32;1m\u001b[1;3mI need to continue searching for more recent papers on variations of multi-head attention models in NLP.\n",
      "Action: arxiv\n",
      "Action Input: multi-head attention NLP variations after 2023\u001b[0m\u001b[36;1m\u001b[1;3mPublished: 2024-08-02\n",
      "Title: Should We Attend More or Less? Modulating Attention for Fairness\n",
      "Authors: Abdelrahman Zayed, Goncalo Mordido, Samira Shabanian, Sarath Chandar\n",
      "Summary: The advances in natural language processing (NLP) pose both opportunities and\n",
      "challenges. While recent progress enables the development of high-performing\n",
      "models for a variety of tasks, it also poses the risk of models learning\n",
      "harmful biases from the data, such as gender stereotypes. In this work, we\n",
      "investigate the role of attention, a widely-used technique in current\n",
      "state-of-the-art NLP models, in the propagation of social biases. Specifically,\n",
      "we study the relationship between the entropy of the attention distribution and\n",
      "the model's performance and fairness. We then propose a novel method for\n",
      "modulating attention weights to improve model fairness after training. Since\n",
      "our method is only applied post-training and pre-inference, it is an\n",
      "intra-processing method and is, therefore, less computationally expensive than\n",
      "existing in-processing and pre-processing approaches. Our results show an\n",
      "increase in fairness and minimal performance loss on different text\n",
      "classification and generation tasks using language models of varying sizes.\n",
      "WARNING: This work uses language that is offensive.\n",
      "\n",
      "Published: 2023-11-26\n",
      "Title: Probabilistic Transformer: A Probabilistic Dependency Model for Contextual Word Representation\n",
      "Authors: Haoyi Wu, Kewei Tu\n",
      "Summary: Syntactic structures used to play a vital role in natural language processing\n",
      "(NLP), but since the deep learning revolution, NLP has been gradually dominated\n",
      "by neural models that do not consider syntactic structures in their design. One\n",
      "vastly successful class of neural models is transformers. When used as an\n",
      "encoder, a transformer produces contextual representation of words in the input\n",
      "sentence. In this work, we propose a new model of contextual word\n",
      "representation, not from a neural perspective, but from a purely syntactic and\n",
      "probabilistic perspective. Specifically, we design a conditional random field\n",
      "that models discrete latent representations of all words in a sentence as well\n",
      "as dependency arcs between them; and we use mean field variational inference\n",
      "for approximate inference. Strikingly, we find that the computation graph of\n",
      "our model resembles transformers, with correspondences between dependencies and\n",
      "self-attention and between distributions over latent representations and\n",
      "contextual embeddings of words. Experiments show that our model performs\n",
      "competitively to transformers on small to medium sized datasets. We hope that\n",
      "our work could help bridge the gap between traditional syntactic and\n",
      "probabilistic approaches and cutting-edge neural approaches to NLP, and inspire\n",
      "more linguistically-principled neural approaches in the future.\n",
      "\n",
      "Published: 2023-02-03\n",
      "Title: Improving Interpretability via Explicit Word Interaction Graph Layer\n",
      "Authors: Arshdeep Sekhon, Hanjie Chen, Aman Shrivastava, Zhe Wang, Yangfeng Ji, Yanjun Qi\n",
      "Summary: Recent NLP literature has seen growing interest in improving model\n",
      "interpretability. Along this direction, we propose a trainable neural network\n",
      "layer that learns a global interaction graph between words and then selects\n",
      "more informative words using the learned word interactions. Our layer, we call\n",
      "WIGRAPH, can plug into any neural network-based NLP text classifiers right\n",
      "after its word embedding layer. Across multiple SOTA NLP models and various NLP\n",
      "datasets, we demonstrate that adding the WIGRAPH layer substantially improves\n",
      "NLP models' interpretability and enhances models' prediction performance at the\n",
      "same time.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/J_Note/lib/python3.9/site-packages/langchain_community/utilities/arxiv.py:103: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  return self.arxiv_search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI have found several recent papers on variations of multi-head attention models in NLP, but none specifically mention models that have come after 2023. I should broaden my search to include all relevant papers on multi-head attention models in NLP.\n",
      "Action: arxiv\n",
      "Action Input: multi-head attention NLP variations\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/J_Note/lib/python3.9/site-packages/langchain_community/utilities/arxiv.py:103: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  return self.arxiv_search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3mPublished: 2025-06-27\n",
      "Title: Quantum-Enhanced Attention Mechanism in NLP: A Hybrid Classical-Quantum Approach\n",
      "Authors: S. M. Yousuf Iqbal Tomal, Abdullah Al Shafin, Debojit Bhattacharjee, MD. Khairul Amin, Rafiad Sadat Shahir\n",
      "Summary: Recent advances in quantum computing have opened new pathways for enhancing\n",
      "deep learning architectures, particularly in domains characterized by\n",
      "high-dimensional and context-rich data such as natural language processing\n",
      "(NLP). In this work, we present a hybrid classical-quantum Transformer model\n",
      "that integrates a quantum-enhanced attention mechanism into the standard\n",
      "classical architecture. By embedding token representations into a quantum\n",
      "Hilbert space via parameterized variational circuits and exploiting\n",
      "entanglement-aware kernel similarities, the model captures complex semantic\n",
      "relationships beyond the reach of conventional dot-product attention. We\n",
      "demonstrate the effectiveness of this approach across diverse NLP benchmarks,\n",
      "showing improvements in both efficiency and representational capacity. The\n",
      "results section reveal that the quantum attention layer yields globally\n",
      "coherent attention maps and more separable latent features, while requiring\n",
      "comparatively fewer parameters than classical counterparts. These findings\n",
      "highlight the potential of quantum-classical hybrid models to serve as a\n",
      "powerful and resource-efficient alternative to existing attention mechanisms in\n",
      "NLP.\n",
      "\n",
      "Published: 2025-05-23\n",
      "Title: HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing\n",
      "Authors: Shamsuddeen Hassan Muhammad, Ibrahim Said Ahmad, Idris Abdulmumin, Falalu Ibrahim Lawan, Babangida Sani, Sukairaj Hafiz Imam, Yusuf Aliyu, Sani Abdullahi Sani, Ali Usman Umar, Tajuddeen Gwadabe, Kenneth Church, Vukosi Marivate\n",
      "Summary: Hausa Natural Language Processing (NLP) has gained increasing attention in\n",
      "recent years, yet remains understudied as a low-resource language despite\n",
      "having over 120 million first-language (L1) and 80 million second-language (L2)\n",
      "speakers worldwide. While significant advances have been made in high-resource\n",
      "languages, Hausa NLP faces persistent challenges, including limited open-source\n",
      "datasets and inadequate model representation. This paper presents an overview\n",
      "of the current state of Hausa NLP, systematically examining existing resources,\n",
      "research contributions, and gaps across fundamental NLP tasks: text\n",
      "classification, machine translation, named entity recognition, speech\n",
      "recognition, and question answering. We introduce HausaNLP\n",
      "(https://catalog.hausanlp.org), a curated catalog that aggregates datasets,\n",
      "tools, and research works to enhance accessibility and drive further\n",
      "development. Furthermore, we discuss challenges in integrating Hausa into large\n",
      "language models (LLMs), addressing issues of suboptimal tokenization and\n",
      "dialectal variation. Finally, we propose strategic research directions\n",
      "emphasizing dataset expansion, improved language modeling approaches, and\n",
      "strengthened community collaboration to advance Hausa NLP. Our work provides\n",
      "both a foundation for accelerating Hausa NLP progress and valuable insights for\n",
      "broader multilingual NLP research.\n",
      "\n",
      "Published: 2023-11-26\n",
      "Title: Probabilistic Transformer: A Probabilistic Dependency Model for Contextual Word Representation\n",
      "Authors: Haoyi Wu, Kewei Tu\n",
      "Summary: Syntactic structures used to play a vital role in natural language processing\n",
      "(NLP), but since the deep learning revolution, NLP has been gradually dominated\n",
      "by neural models that do not consider syntactic structures in their design. One\n",
      "vastly successful class of neural models is transformers. When used as an\n",
      "encoder, a transformer produces contextual representation of words in the input\n",
      "sentence. In this work, we propose a new model of contextual word\n",
      "representation, not from a neural perspective, but from a purely syntactic and\n",
      "probabilistic perspective. Specifically, we design a conditional random field\n",
      "that models discrete latent representation\u001b[0m\u001b[32;1m\u001b[1;3mI have found some recent papers on variations of multi-head attention models in NLP, including a quantum-enhanced attention mechanism and a model for Hausa NLP. There are no specific models mentioned that have come after 2023. I will search for more recent papers to find the complete list of models.\n",
      "Action: arxiv\n",
      "Action Input: multi-head attention NLP variations\u001b[0m\u001b[36;1m\u001b[1;3mPublished: 2025-06-27\n",
      "Title: Quantum-Enhanced Attention Mechanism in NLP: A Hybrid Classical-Quantum Approach\n",
      "Authors: S. M. Yousuf Iqbal Tomal, Abdullah Al Shafin, Debojit Bhattacharjee, MD. Khairul Amin, Rafiad Sadat Shahir\n",
      "Summary: Recent advances in quantum computing have opened new pathways for enhancing\n",
      "deep learning architectures, particularly in domains characterized by\n",
      "high-dimensional and context-rich data such as natural language processing\n",
      "(NLP). In this work, we present a hybrid classical-quantum Transformer model\n",
      "that integrates a quantum-enhanced attention mechanism into the standard\n",
      "classical architecture. By embedding token representations into a quantum\n",
      "Hilbert space via parameterized variational circuits and exploiting\n",
      "entanglement-aware kernel similarities, the model captures complex semantic\n",
      "relationships beyond the reach of conventional dot-product attention. We\n",
      "demonstrate the effectiveness of this approach across diverse NLP benchmarks,\n",
      "showing improvements in both efficiency and representational capacity. The\n",
      "results section reveal that the quantum attention layer yields globally\n",
      "coherent attention maps and more separable latent features, while requiring\n",
      "comparatively fewer parameters than classical counterparts. These findings\n",
      "highlight the potential of quantum-classical hybrid models to serve as a\n",
      "powerful and resource-efficient alternative to existing attention mechanisms in\n",
      "NLP.\n",
      "\n",
      "Published: 2025-05-23\n",
      "Title: HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing\n",
      "Authors: Shamsuddeen Hassan Muhammad, Ibrahim Said Ahmad, Idris Abdulmumin, Falalu Ibrahim Lawan, Babangida Sani, Sukairaj Hafiz Imam, Yusuf Aliyu, Sani Abdullahi Sani, Ali Usman Umar, Tajuddeen Gwadabe, Kenneth Church, Vukosi Marivate\n",
      "Summary: Hausa Natural Language Processing (NLP) has gained increasing attention in\n",
      "recent years, yet remains understudied as a low-resource language despite\n",
      "having over 120 million first-language (L1) and 80 million second-language (L2)\n",
      "speakers worldwide. While significant advances have been made in high-resource\n",
      "languages, Hausa NLP faces persistent challenges, including limited open-source\n",
      "datasets and inadequate model representation. This paper presents an overview\n",
      "of the current state of Hausa NLP, systematically examining existing resources,\n",
      "research contributions, and gaps across fundamental NLP tasks: text\n",
      "classification, machine translation, named entity recognition, speech\n",
      "recognition, and question answering. We introduce HausaNLP\n",
      "(https://catalog.hausanlp.org), a curated catalog that aggregates datasets,\n",
      "tools, and research works to enhance accessibility and drive further\n",
      "development. Furthermore, we discuss challenges in integrating Hausa into large\n",
      "language models (LLMs), addressing issues of suboptimal tokenization and\n",
      "dialectal variation. Finally, we propose strategic research directions\n",
      "emphasizing dataset expansion, improved language modeling approaches, and\n",
      "strengthened community collaboration to advance Hausa NLP. Our work provides\n",
      "both a foundation for accelerating Hausa NLP progress and valuable insights for\n",
      "broader multilingual NLP research.\n",
      "\n",
      "Published: 2023-11-26\n",
      "Title: Probabilistic Transformer: A Probabilistic Dependency Model for Contextual Word Representation\n",
      "Authors: Haoyi Wu, Kewei Tu\n",
      "Summary: Syntactic structures used to play a vital role in natural language processing\n",
      "(NLP), but since the deep learning revolution, NLP has been gradually dominated\n",
      "by neural models that do not consider syntactic structures in their design. One\n",
      "vastly successful class of neural models is transformers. When used as an\n",
      "encoder, a transformer produces contextual representation of words in the input\n",
      "sentence. In this work, we propose a new model of contextual word\n",
      "representation, not from a neural perspective, but from a purely syntactic and\n",
      "probabilistic perspective. Specifically, we design a conditional random field\n",
      "that models discrete latent representation\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/J_Note/lib/python3.9/site-packages/langchain_community/utilities/arxiv.py:103: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  return self.arxiv_search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI have found some recent papers on variations of multi-head attention models in NLP, including a quantum-enhanced attention mechanism and a model for Hausa NLP. There are no specific models mentioned that have come after 2023. I will search for more recent papers to find the complete list of models.\n",
      "Action: arxiv\n",
      "Action Input: multi-head attention NLP variations\u001b[0m\u001b[36;1m\u001b[1;3mPublished: 2025-06-27\n",
      "Title: Quantum-Enhanced Attention Mechanism in NLP: A Hybrid Classical-Quantum Approach\n",
      "Authors: S. M. Yousuf Iqbal Tomal, Abdullah Al Shafin, Debojit Bhattacharjee, MD. Khairul Amin, Rafiad Sadat Shahir\n",
      "Summary: Recent advances in quantum computing have opened new pathways for enhancing\n",
      "deep learning architectures, particularly in domains characterized by\n",
      "high-dimensional and context-rich data such as natural language processing\n",
      "(NLP). In this work, we present a hybrid classical-quantum Transformer model\n",
      "that integrates a quantum-enhanced attention mechanism into the standard\n",
      "classical architecture. By embedding token representations into a quantum\n",
      "Hilbert space via parameterized variational circuits and exploiting\n",
      "entanglement-aware kernel similarities, the model captures complex semantic\n",
      "relationships beyond the reach of conventional dot-product attention. We\n",
      "demonstrate the effectiveness of this approach across diverse NLP benchmarks,\n",
      "showing improvements in both efficiency and representational capacity. The\n",
      "results section reveal that the quantum attention layer yields globally\n",
      "coherent attention maps and more separable latent features, while requiring\n",
      "comparatively fewer parameters than classical counterparts. These findings\n",
      "highlight the potential of quantum-classical hybrid models to serve as a\n",
      "powerful and resource-efficient alternative to existing attention mechanisms in\n",
      "NLP.\n",
      "\n",
      "Published: 2025-05-23\n",
      "Title: HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing\n",
      "Authors: Shamsuddeen Hassan Muhammad, Ibrahim Said Ahmad, Idris Abdulmumin, Falalu Ibrahim Lawan, Babangida Sani, Sukairaj Hafiz Imam, Yusuf Aliyu, Sani Abdullahi Sani, Ali Usman Umar, Tajuddeen Gwadabe, Kenneth Church, Vukosi Marivate\n",
      "Summary: Hausa Natural Language Processing (NLP) has gained increasing attention in\n",
      "recent years, yet remains understudied as a low-resource language despite\n",
      "having over 120 million first-language (L1) and 80 million second-language (L2)\n",
      "speakers worldwide. While significant advances have been made in high-resource\n",
      "languages, Hausa NLP faces persistent challenges, including limited open-source\n",
      "datasets and inadequate model representation. This paper presents an overview\n",
      "of the current state of Hausa NLP, systematically examining existing resources,\n",
      "research contributions, and gaps across fundamental NLP tasks: text\n",
      "classification, machine translation, named entity recognition, speech\n",
      "recognition, and question answering. We introduce HausaNLP\n",
      "(https://catalog.hausanlp.org), a curated catalog that aggregates datasets,\n",
      "tools, and research works to enhance accessibility and drive further\n",
      "development. Furthermore, we discuss challenges in integrating Hausa into large\n",
      "language models (LLMs), addressing issues of suboptimal tokenization and\n",
      "dialectal variation. Finally, we propose strategic research directions\n",
      "emphasizing dataset expansion, improved language modeling approaches, and\n",
      "strengthened community collaboration to advance Hausa NLP. Our work provides\n",
      "both a foundation for accelerating Hausa NLP progress and valuable insights for\n",
      "broader multilingual NLP research.\n",
      "\n",
      "Published: 2023-11-26\n",
      "Title: Probabilistic Transformer: A Probabilistic Dependency Model for Contextual Word Representation\n",
      "Authors: Haoyi Wu, Kewei Tu\n",
      "Summary: Syntactic structures used to play a vital role in natural language processing\n",
      "(NLP), but since the deep learning revolution, NLP has been gradually dominated\n",
      "by neural models that do not consider syntactic structures in their design. One\n",
      "vastly successful class of neural models is transformers. When used as an\n",
      "encoder, a transformer produces contextual representation of words in the input\n",
      "sentence. In this work, we propose a new model of contextual word\n",
      "representation, not from a neural perspective, but from a purely syntactic and\n",
      "probabilistic perspective. Specifically, we design a conditional random field\n",
      "that models discrete latent representation\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/J_Note/lib/python3.9/site-packages/langchain_community/utilities/arxiv.py:103: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  return self.arxiv_search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI have found some recent papers on variations of multi-head attention models in NLP, including a quantum-enhanced attention mechanism and a model for Hausa NLP. There are no specific models mentioned that have come after 2023. I will search for more recent papers to find the complete list of models.\n",
      "Action: arxiv\n",
      "Action Input: multi-head attention NLP variations\u001b[0m\u001b[36;1m\u001b[1;3mPublished: 2025-06-27\n",
      "Title: Quantum-Enhanced Attention Mechanism in NLP: A Hybrid Classical-Quantum Approach\n",
      "Authors: S. M. Yousuf Iqbal Tomal, Abdullah Al Shafin, Debojit Bhattacharjee, MD. Khairul Amin, Rafiad Sadat Shahir\n",
      "Summary: Recent advances in quantum computing have opened new pathways for enhancing\n",
      "deep learning architectures, particularly in domains characterized by\n",
      "high-dimensional and context-rich data such as natural language processing\n",
      "(NLP). In this work, we present a hybrid classical-quantum Transformer model\n",
      "that integrates a quantum-enhanced attention mechanism into the standard\n",
      "classical architecture. By embedding token representations into a quantum\n",
      "Hilbert space via parameterized variational circuits and exploiting\n",
      "entanglement-aware kernel similarities, the model captures complex semantic\n",
      "relationships beyond the reach of conventional dot-product attention. We\n",
      "demonstrate the effectiveness of this approach across diverse NLP benchmarks,\n",
      "showing improvements in both efficiency and representational capacity. The\n",
      "results section reveal that the quantum attention layer yields globally\n",
      "coherent attention maps and more separable latent features, while requiring\n",
      "comparatively fewer parameters than classical counterparts. These findings\n",
      "highlight the potential of quantum-classical hybrid models to serve as a\n",
      "powerful and resource-efficient alternative to existing attention mechanisms in\n",
      "NLP.\n",
      "\n",
      "Published: 2025-05-23\n",
      "Title: HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing\n",
      "Authors: Shamsuddeen Hassan Muhammad, Ibrahim Said Ahmad, Idris Abdulmumin, Falalu Ibrahim Lawan, Babangida Sani, Sukairaj Hafiz Imam, Yusuf Aliyu, Sani Abdullahi Sani, Ali Usman Umar, Tajuddeen Gwadabe, Kenneth Church, Vukosi Marivate\n",
      "Summary: Hausa Natural Language Processing (NLP) has gained increasing attention in\n",
      "recent years, yet remains understudied as a low-resource language despite\n",
      "having over 120 million first-language (L1) and 80 million second-language (L2)\n",
      "speakers worldwide. While significant advances have been made in high-resource\n",
      "languages, Hausa NLP faces persistent challenges, including limited open-source\n",
      "datasets and inadequate model representation. This paper presents an overview\n",
      "of the current state of Hausa NLP, systematically examining existing resources,\n",
      "research contributions, and gaps across fundamental NLP tasks: text\n",
      "classification, machine translation, named entity recognition, speech\n",
      "recognition, and question answering. We introduce HausaNLP\n",
      "(https://catalog.hausanlp.org), a curated catalog that aggregates datasets,\n",
      "tools, and research works to enhance accessibility and drive further\n",
      "development. Furthermore, we discuss challenges in integrating Hausa into large\n",
      "language models (LLMs), addressing issues of suboptimal tokenization and\n",
      "dialectal variation. Finally, we propose strategic research directions\n",
      "emphasizing dataset expansion, improved language modeling approaches, and\n",
      "strengthened community collaboration to advance Hausa NLP. Our work provides\n",
      "both a foundation for accelerating Hausa NLP progress and valuable insights for\n",
      "broader multilingual NLP research.\n",
      "\n",
      "Published: 2023-11-26\n",
      "Title: Probabilistic Transformer: A Probabilistic Dependency Model for Contextual Word Representation\n",
      "Authors: Haoyi Wu, Kewei Tu\n",
      "Summary: Syntactic structures used to play a vital role in natural language processing\n",
      "(NLP), but since the deep learning revolution, NLP has been gradually dominated\n",
      "by neural models that do not consider syntactic structures in their design. One\n",
      "vastly successful class of neural models is transformers. When used as an\n",
      "encoder, a transformer produces contextual representation of words in the input\n",
      "sentence. In this work, we propose a new model of contextual word\n",
      "representation, not from a neural perspective, but from a purely syntactic and\n",
      "probabilistic perspective. Specifically, we design a conditional random field\n",
      "that models discrete latent representation\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/J_Note/lib/python3.9/site-packages/langchain_community/utilities/arxiv.py:103: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  return self.arxiv_search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mAfter searching through multiple papers on variations of multi-head attention models in NLP, including recent ones, I was unable to find specific models that have come out after 2023. The models mentioned are either from 2023 or 2024. It seems that there may not be many papers specifically mentioning models that have come out after 2023. I will summarize the information I have gathered so far.\n",
      "Final Answer: There are variations of multi-head attention models in NLP, such as quantum-enhanced attention mechanisms and models for specific languages like Hausa NLP, but there are no specific models mentioned that have come out after 2023 in the papers searched in arxiv.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 6. 사용자 질문 실행\n",
    "query = \"2023년 이후로 나온 멀티헤드어텐션의 변형 모델들의 종류는?\"\n",
    "result = agent_executor.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c17b8217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "답변:\n",
      " There are variations of multi-head attention models in NLP, such as quantum-enhanced attention mechanisms and models for specific languages like Hausa NLP, but there are no specific models mentioned that have come out after 2023 in the papers searched in arxiv.\n"
     ]
    }
   ],
   "source": [
    "# 7. 결과 출력\n",
    "print(\"\\n답변:\\n\", result[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee66e4e",
   "metadata": {},
   "source": [
    "<흐름>\n",
    "- Question: 2023년 이후로 나온 멀티헤드어텐션의 변형 모델들의 종류는?\n",
    "\n",
    "- Thought: 최신 논문에서 관련 모델을 찾아야 합니다.\n",
    "- Action: arxiv\n",
    "- Action Input: multi-head attention variants after 2023\n",
    "- Observation: 논문 목록 반환 (Linformer++, Hybrid MHA 등...)\n",
    "\n",
    "- Thought: 논문들을 참고해서 모델 이름을 정리합니다.\n",
    "Final Answer: 2023년 이후 등장한 변형 모델에는 Linformer++, Hybrid Global-Local Attention 등이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0378bc81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fbe2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03078dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "J_Note",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
