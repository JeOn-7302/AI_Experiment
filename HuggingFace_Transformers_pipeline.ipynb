{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c62954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a6772d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.41.2\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /opt/anaconda3/envs/J_Note/lib/python3.9/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: sentence-transformers\n",
      "---\n",
      "Name: tokenizers\n",
      "Version: 0.19.1\n",
      "Summary: \n",
      "Home-page: https://github.com/huggingface/tokenizers\n",
      "Author: Anthony MOI <m.anthony.moi@gmail.com>\n",
      "Author-email: Nicolas Patry <patry.nicolas@protonmail.com>, Anthony Moi <anthony@huggingface.co>\n",
      "License: \n",
      "Location: /opt/anaconda3/envs/J_Note/lib/python3.9/site-packages\n",
      "Requires: huggingface-hub\n",
      "Required-by: langchain-huggingface, transformers\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip show transformers tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52d1954",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda793e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers==4.41.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ddff52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0709 23:23:30.834162 38632 site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae997790",
   "metadata": {},
   "source": [
    "## 감정 분석 (P/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59bdac07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998536109924316}]\n"
     ]
    }
   ],
   "source": [
    "# 감정 분석 파이프라인 생성\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# 텍스트 분석\n",
    "result = sentiment_analyzer(\"I’m so proud of you.\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61903c7",
   "metadata": {},
   "source": [
    "## 텍스트 분류 (Text Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7c1fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/opt/anaconda3/envs/J_Note/lib/python3.9/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9992406368255615}]\n"
     ]
    }
   ],
   "source": [
    "# 감정 이진 분석\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"Don't give up\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1a51fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'joy', 'score': 0.3598346710205078}]\n"
     ]
    }
   ],
   "source": [
    "# 다중 레이블 분류\n",
    "classifier = pipeline(\"text-classification\", model=\"bhadresh-savani/distilbert-base-uncased-emotion\")\n",
    "result = classifier(\"It felt like everything was falling into place.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357772c2",
   "metadata": {},
   "source": [
    "## 텍스트 생성 (Text Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69cfdda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 6c0e608 (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/opt/anaconda3/envs/J_Note/lib/python3.9/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'No one expected what would happen that day. But then the president made that speech and I was sitting in the conference room,\" he said. \"And'}, {'generated_text': 'No one expected what would happen that day. It took the cops a few hours to clear away one of the buildings on the hillside, though you'}]\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 자동 완성\n",
    "generator = pipeline(\"text-generation\")\n",
    "result = generator(\"No one expected what would happen that day.\", max_length=30, num_return_sequences=2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e92fae35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI is going to work through how that will work. They are really excited about it.\n",
      "\n",
      "JASON HOGG: So you are bringing in the two top-level people, two of the people closest to you, not just the CEO, right? [audience laughs]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 특정 모델 지정\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2-medium\")\n",
    "result = generator(\"AI is going to\", max_length=60)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3edd3f2",
   "metadata": {},
   "source": [
    "## 질의응답 (Question Answering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c233d3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.0031824710313230753, 'start': 172, 'end': 233, 'answer': '이 에너지를 이용해 물과 이산화탄소가 포도당으로 바뀝니다.\\n광합성은 지구 생태계에서 매우 중요한 역할을 합니다'}\n"
     ]
    }
   ],
   "source": [
    "# 지문 기반 질의응답\n",
    "qa = pipeline(\"question-answering\")\n",
    "context = \"\"\"식물은 광합성을 통해 스스로 양분을 만들어내는 생물입니다.\n",
    "광합성은 주로 잎에서 일어나며, 식물은 햇빛, 이산화탄소, 물을 이용하여 포도당과 산소를 만들어냅니다.\n",
    "이 과정에서 식물은 뿌리를 통해 물을 흡수하고, 잎의 기공을 통해 이산화탄소를 받아들입니다.\n",
    "햇빛은 잎의 엽록체에서 흡수되어 에너지로 전환되며, 이 에너지를 이용해 물과 이산화탄소가 포도당으로 바뀝니다.\n",
    "광합성은 지구 생태계에서 매우 중요한 역할을 합니다.\n",
    "왜냐하면 동물과 사람은 식물이 만든 산소를 숨 쉬는 데 사용하고, 식물이 만든 유기물을 먹고 살아가기 때문입니다.\n",
    "\"\"\"\n",
    "\n",
    "result = qa(question=\"광합성의 산물로 만들어지는 두 가지는 무엇인가요?\", context=context)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753de17d",
   "metadata": {},
   "source": [
    "## 개체명 인식 (Named Entity Recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efb986b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'ORG', 'score': np.float32(0.9990183), 'word': 'Apple', 'start': 0, 'end': 5}, {'entity_group': 'LOC', 'score': np.float32(0.9996722), 'word': 'U', 'start': 27, 'end': 28}, {'entity_group': 'LOC', 'score': np.float32(0.9979365), 'word': 'K', 'start': 29, 'end': 30}]\n"
     ]
    }
   ],
   "source": [
    "# 텍스트에서 개체명 추출\n",
    "ner = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "result = ner(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5767b139",
   "metadata": {},
   "source": [
    "## 요약 (Summarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6940d33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대기 중 온실가스 농도의 증가로 지구 평균 기온이 상승하는 현상이며, 이러한 기온 상승은 어업에도 영향을 미쳐 수산물의 온도가 높아지면 산호초가 하얗게 변하는 백화 현상이 발생하고 심할 경우 산호는 죽게 만들고 기상청은 산호는 죽게 만드는 등의\n"
     ]
    }
   ],
   "source": [
    "# 긴 텍스트 요약\n",
    "summarizer = pipeline(\"summarization\", model=\"digit82/kobart-summarization\")\n",
    "long_text = \"\"\"\n",
    "지구 온난화는 대기 중 온실가스 농도의 증가로 인해 지구 평균 기온이 상승하는 현상이다. 이러한 기온 상승은 해양에도 큰 영향을 미친다.\n",
    "대표적인 예로, 바닷물의 온도가 높아지면 산호초가 하얗게 변하는 백화 현상이 나타난다.\n",
    "이는 산호가 스트레스를 받아 몸속에 공생하던 조류를 내보내기 때문에 발생하며, 심할 경우 산호는 죽게 된다.\n",
    "\n",
    "또한 수온 상승은 해양 생물의 서식지를 바꾸기도 한다.\n",
    "일부 어류는 차가운 물을 찾아 더 북쪽이나 깊은 바다로 이동하며, 이는 어업에도 영향을 미친다.\n",
    "해양의 산성화도 문제이다.\n",
    "대기 중 이산화탄소가 바닷물에 녹으면 탄산이 생성되어 바다의 pH가 낮아지고, 조개나 산호처럼 석회질 껍데기를 만드는 생물들은 성장에 어려움을 겪는다.\n",
    "\"\"\"\n",
    "result = summarizer(long_text, max_length=60, min_length=30, do_sample=False)\n",
    "print(result[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195b3f7e",
   "metadata": {},
   "source": [
    "## 번역 (Translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e37eb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-base and revision 686f1db (https://huggingface.co/google-t5/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je vous aime!\n"
     ]
    }
   ],
   "source": [
    "# 영어에서 프랑스어로 번역\n",
    "translator = pipeline(\"translation_en_to_fr\")\n",
    "result = translator(\"I love you!\")\n",
    "print(result[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aaafa784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인공지능은 인간의 사고와 의사결정을 모방할 수 있는 컴퓨터 시스템을 말합니다.\n"
     ]
    }
   ],
   "source": [
    "# 다른 언어쌍 지정\n",
    "translator = pipeline(\"translation\", model=\"facebook/nllb-200-distilled-600M\", src_lang=\"eng_Latn\", tgt_lang=\"kor_Hang\")\n",
    "result = translator(\"Artificial intelligence refers to computer systems that can mimic human thinking and decision-making.\")\n",
    "print(result[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce396b43",
   "metadata": {},
   "source": [
    "## 이미지 분류 (Image Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1d6460db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5a6a9fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google/vit-base-patch16-224 and revision 5dca96d (https://huggingface.co/google/vit-base-patch16-224).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens', 'score': 0.9988446235656738}, {'label': 'polecat, fitch, foulmart, foumart, Mustela putorius', 'score': 0.00020459223014768213}, {'label': 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca', 'score': 8.872239413904026e-05}, {'label': 'weasel', 'score': 4.114701005164534e-05}, {'label': 'mink', 'score': 2.4508290152880363e-05}]\n"
     ]
    }
   ],
   "source": [
    "# 이미지 URL에서 이미지 다운로드\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e6/Red_Panda_%2824986761703%29.jpg/960px-Red_Panda_%2824986761703%29.jpg\"\n",
    "image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "\n",
    "# 이미지 분류\n",
    "image_classifier = pipeline(\"image-classification\")\n",
    "result = image_classifier(image)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425f1e2e",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e6/Red_Panda_%2824986761703%29.jpg/2560px-Red_Panda_%2824986761703%29.jpg\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfe8f35",
   "metadata": {},
   "source": [
    "## 음성 인식 (Automatic Speech Recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "804ccc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 코로나19 예방수칙입니다. 손을 자주 씻기, 마스크 착용하기, 기침할 때는 입과 코 가리기, 발열, 기침, 인후통 등 증상 의심 시에는 1339 또는 보건소와 상담하시기 바랍니다.\n"
     ]
    }
   ],
   "source": [
    "# 음성에서 텍스트로 변환\n",
    "transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-small\", chunk_length_s=30, generate_kwargs={\"language\": \"<|ko|>\"})\n",
    "\n",
    "audio_file = \"./data/COVID19_ShortAnnouncement.wav\"\n",
    "result = transcriber(audio_file)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e38d97",
   "metadata": {},
   "source": [
    "## 커스텀 모델 및 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "47340ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The phone rang just once. Then silence.\\n\\n\"Hey, I\\'m having a hard time trying to sleep,\" she said.\\n\\n\"I\\'m sorry,\" he said. \"I\\'m just here to ask you, okay?\"\\n\\n\"No,\" she said. \"I just… I just… just want you to know that I\\'m really happy for you.\"\\n\\nHe could hear the phone ring again, and she looked away. He tried to hold back tears. He remembered'}, {'generated_text': 'The phone rang just once. Then silence.\\n\\n\"Well, I hope so,\" she said. \"I think you\\'ve got it all figured out.\"\\n\\n\"You have to find out,\" I said. \"Why should I bother trying to explain it to you?\"\\n\\n\"It\\'s not that I don\\'t want to understand it,\" she said. \"I mean, I do. I\\'ve spent quite a bit of time here lately.\"\\n\\n\"Oh, yeah. I'}, {'generated_text': 'The phone rang just once. Then silence. I walked to the door, and, just as I had expected, she opened it.\\n\\n\"What is it?\" I asked, \"you don\\'t know. Why did you bring it here?\"\\n\\nShe shook her head, and I looked up to see her smiling. \"I thought you might want to have a look.\"\\n\\n\"Yeah,\" I said. \"I thought it would be nice to have a look at you.\"\\n'}]\n"
     ]
    }
   ],
   "source": [
    "# 특정 모델 지정\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    ")\n",
    "\n",
    "# 장치 지정 (CPU/GPU)\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\", device=0)  # 0번 GPU 사용\n",
    "\n",
    "# 다양한 파라미터 설정\n",
    "result = generator(\n",
    "    \"The phone rang just once. Then silence.\",\n",
    "    max_length=100,          # 생성할 텍스트의 최대 길이 (토큰 수 기준)\n",
    "    num_return_sequences=3,  # 몇 개의 서로 다른 결과(문장)를 생성할지 지정\n",
    "    temperature=0.7,         # 낮을수록 확정적(예측 확률 높은 단어 위주), 높을수록 창의적\n",
    "    top_k=50,                # 상위 50개 토큰 중에서만 샘플링 (랜덤 선택 범위 제한)\n",
    "    top_p=0.95,              # 누적 확률이 95% 넘는 토큰까지만 고려 (nucleus sampling)\n",
    "    do_sample=True,          # 샘플링 기반 텍스트 생성 활성화 (True일 때만 top_k/top_p/temperature 작동)\n",
    "    truncation=True,         # 입력 시퀀스가 max_length를 초과하면 잘라냄 (오버플로 방지)\n",
    "    pad_token_id=50256       # 패딩 토큰이 없을 경우 사용할 종료 토큰 ID (GPT2의 eos token)\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727dfe60",
   "metadata": {},
   "source": [
    "## 배치 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7d426732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트: 'The weather today is sunny and warm.'\n",
      "감정: 4 stars, 점수: 0.4261\n",
      "\n",
      "텍스트: 'I had a terrible experience at the restaurant.'\n",
      "감정: 1 star, 점수: 0.6630\n",
      "\n",
      "텍스트: 'Can you help me find the nearest subway station?'\n",
      "감정: 3 stars, 점수: 0.2596\n",
      "\n",
      "텍스트: 'This movie is absolutely fantastic and inspiring.'\n",
      "감정: 5 stars, 점수: 0.9433\n",
      "\n",
      "텍스트: 'I'm feeling a bit tired after a long day of work.'\n",
      "감정: 3 stars, 점수: 0.5272\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여러 텍스트 배치 처리\n",
    "texts = [\n",
    "    \"The weather today is sunny and warm.\",\n",
    "    \"I had a terrible experience at the restaurant.\",\n",
    "    \"Can you help me find the nearest subway station?\",\n",
    "    \"This movie is absolutely fantastic and inspiring.\",\n",
    "    \"I'm feeling a bit tired after a long day of work.\"\n",
    "]\n",
    "results = sentiment_analyzer(texts)\n",
    "for text, result in zip(texts, results):\n",
    "    print(f\"텍스트: '{text}'\")\n",
    "    print(f\"감정: {result['label']}, 점수: {result['score']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa587dc",
   "metadata": {},
   "source": [
    "## 토크나이저 및 모델 세부 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "664e8038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "779043f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.999344527721405}] [{'label': 'POSITIVE', 'score': 0.9998782873153687}]\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저와 모델 직접 로드\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# 커스텀 파이프라인 생성\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# 긴 텍스트 처리를 위한 설정\n",
    "classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "result_01 = classifier(\"It felt like everything was falling into place.\")\n",
    "result_02 = classifier(\"I love you!\")\n",
    "\n",
    "print(result_01, result_02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89babe3e",
   "metadata": {},
   "source": [
    "## 유지 및 관리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d60a35",
   "metadata": {},
   "source": [
    "### 모델 캐싱 및 관리\n",
    "필요하면 캐시 위치 바꾸고, 이미 다운로드한 모델을 로컬에서 안전하게 재사용할 때 유용한 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838b583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 모델 캐시 디렉토리 설정\n",
    "# TRANSFORMERS_CACHE 환경변수로 모델 캐시 위치를 지정해 원하는 폴더에 저장/읽기 가능\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/path/to/cache\"\n",
    "\n",
    "# 오프라인 모드 활성화\n",
    "# local_files_only=True를 사용하면 인터넷 연결 없이 로컬에 있는 모델만 사용함 (다운로드 시도하지 않음)\n",
    "# pipeline 함수로 원하는 작업과 모델을 지정해 바로 추론 파이프라인을 만들 수 있음\n",
    "classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acc43d9",
   "metadata": {},
   "source": [
    "### 모델 양자화로 메모리 사용량 줄이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83334197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BitsAndBytesConfig 라이브러리 : Transformers에서 제공하는, 양자화(quantization) 설정을 위한 클래스\n",
    "from transformers import pipeline, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# 4비트 양자화 설정\n",
    "# 4비트 양자화는 모델의 메모리 사용량을 크게 줄이고, 연산 속도를 높이는 효과가 있지만, 약간의 정확도 손실이 있을 수 있음.\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,    # 모델 파라미터를 4비트 정밀도로 불러오겠다라는 의미(일반적으로 32비트 부동소수점)\n",
    "    bnb_4bit_compute_dtype=torch.float16    \n",
    "    # 4비트로 압축된 모델을 계산할 때 내부 연산은 반정밀도(16비트 부동소수점)로 수행하겠다는 의미\n",
    "    # 정확도와 속도 균형을 맞추기 위해 float16 연산을 쓰는 경우\n",
    ")\n",
    "\n",
    "# 양자화된 모델 로드\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"bigscience/bloom-1b7\",\n",
    "    quantization_config=quantization_config    # 앞서 만든 4비트 양자화 설정을 적용하여 모델을 로드\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be1a65b",
   "metadata": {},
   "source": [
    "### 배치 크기 및 병렬 처리 최적화\n",
    "\n",
    "배치 크기가 크면 GPU를 효율적으로 활용해 처리 속도가 빨라질 수 있지만 너무 크면 메모리 부족이나 오버헤드로 속도가 느려질 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8983d64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import time\n",
    "\n",
    "classifier = pipeline(\"text-classification\", device=0)  # GPU 사용\n",
    "\n",
    "# 배치 크기 실험\n",
    "# 처리할 문장 1000개를 준비 <- 배치 처리 성능 실험에 적합하도록 같은 데이터를 반복 사용\n",
    "texts = [\"Sample text\"] * 1000\n",
    "\n",
    "# 실험할 배치 크기 리스트 - 한 번에 처리하는 텍스트 개수를 1개부터 64개까지 바꿔가며 측정\n",
    "batch_sizes = [4, 8, 16, 32, 64]\n",
    "\n",
    "# 각 배치 크기에 대해 반복\n",
    "for batch_size in batch_sizes:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # classifier를 호출할 때 batch_size=batch_size 인자를 주어, 한 번에 처리할 문장 수를 조절\n",
    "    _ = classifier(texts, batch_size=batch_size)\n",
    "    duration = time.time() - start_time    # 처리 후 걸린 시간 계산\n",
    "    print(f\"배치 크기 {batch_size}: {duration:.2f}초 소요\")    # 배치 크기와 처리 시간을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e506027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441886ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d3b7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226b8214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(J_Note)",
   "language": "python",
   "name": "j_note"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
